Creating cluster...
Waiting for cluster creation...
Cluster created.
Uploading pyspark file to GCS
hw5-2 - RUNNING
Submitted job ID 4d465fa8-1d09-4699-a228-c468d120840c
Waiting for job to finish...
Job finished.
Downloading output file
Received job output b"Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-xml_2.11 added as a dependency\ngraphframes#graphframes added as a dependency\ncom.databricks#spark-avro_2.11 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-79577a56-d3b6-4839-9944-0bda47a5ba3c;1.0\n\tconfs: [default]\n\tfound com.databricks#spark-xml_2.11;0.4.1 in central\n\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\n\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n\tfound org.scala-lang#scala-reflect;2.11.0 in central\n\tfound org.slf4j#slf4j-api;1.7.7 in central\n\tfound com.databricks#spark-avro_2.11;4.0.0 in central\n\tfound org.apache.avro#avro;1.7.6 in central\n\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n\tfound org.xerial.snappy#snappy-java;1.0.5 in central\n\tfound org.apache.commons#commons-compress;1.4.1 in central\n\tfound org.tukaani#xz;1.0 in central\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\n\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (32ms)\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\n\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (264ms)\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\n\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (13ms)\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\n\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (11ms)\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\n\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (11ms)\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\n\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (54ms)\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\n\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (12ms)\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\n\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (13ms)\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (12ms)\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (15ms)\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\n\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (11ms)\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\n\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (16ms)\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\n\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (16ms)\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\n\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (11ms)\n:: resolution report :: resolve 2964ms :: artifacts dl 500ms\n\t:: modules in use:\n\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\n\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\n\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\n\torg.apache.avro#avro;1.7.6 from central in [default]\n\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n\torg.tukaani#xz;1.0 from central in [default]\n\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\n\t:: evicted modules:\n\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\n\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-79577a56-d3b6-4839-9944-0bda47a5ba3c\n\tconfs: [default]\n\t14 artifacts copied, 0 already retrieved (7998kB/26ms)\n19/11/05 19:50:28 INFO org.spark_project.jetty.util.log: Logging initialized @8884ms\n19/11/05 19:50:28 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n19/11/05 19:50:28 INFO org.spark_project.jetty.server.Server: Started @8964ms\n19/11/05 19:50:28 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@30ad5d97{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n19/11/05 19:50:28 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n19/11/05 19:50:29 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at hw5-2-m/10.128.0.22:8032\n19/11/05 19:50:29 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at hw5-2-m/10.128.0.22:10200\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.11-0.4.1.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.avro_avro-1.7.6.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar added multiple times to distributed cache.\n19/11/05 19:50:31 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.tukaani_xz-1.0.jar added multiple times to distributed cache.\n19/11/05 19:50:32 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1572983329756_0001\n19/11/05 19:50:51 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n('... full graph initialized in', 460.2182939052582, ' seconds.')\n19/11/05 20:31:38 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on hw5-2-w-0.c.adroit-chemist-229001.internal: Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 14.0 in stage 48.0 (TID 286, hw5-2-w-0.c.adroit-chemist-229001.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 13.0 in stage 48.0 (TID 285, hw5-2-w-0.c.adroit-chemist-229001.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 48.0 (TID 284, hw5-2-w-0.c.adroit-chemist-229001.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 15.0 in stage 48.0 (TID 287, hw5-2-w-0.c.adroit-chemist-229001.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.0 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:31:38 WARN org.apache.spark.ExecutorAllocationManager: Attempted to mark unknown executor 1 idle\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_13 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_12 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_15 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_3 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_0 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_14 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_1 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_2 !\n19/11/05 20:31:38 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_3 !\n19/11/05 20:31:38 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /10.128.0.23:34516\njava.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)\n\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\n19/11/05 20:47:00 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 5 on hw5-2-w-1.c.adroit-chemist-229001.internal: Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 8.0 in stage 80.0 (TID 378, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 10.0 in stage 80.0 (TID 381, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4.0 in stage 80.0 (TID 372, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 6.0 in stage 80.0 (TID 375, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.3 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_7 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_8 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_7 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_5 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_5 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_4 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_6 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_4 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_8 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_7 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_7 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_10 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_10 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_10 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_8 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_6 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_11 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_10 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_8 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_5 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_9 !\n19/11/05 20:47:00 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_5 !\n19/11/05 20:47:00 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /10.128.0.24:34302\njava.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)\n\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\n19/11/05 20:47:27 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on hw5-2-w-1.c.adroit-chemist-229001.internal: Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 5.0 in stage 80.0 (TID 373, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 11.0 in stage 80.0 (TID 382, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 7.0 in stage 80.0 (TID 376, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 9.0 in stage 80.0 (TID 379, hw5-2-w-1.c.adroit-chemist-229001.internal, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 12.1 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_10 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_6 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_8 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_6 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_11 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_6 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_6 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_5 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_8 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_5 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_9 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_8 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_7 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_11 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_9 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_38_10 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_8 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_7 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_6 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_52_10 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_7 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_45_15 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_17_5 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_4 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_24_7 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_31_10 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_10_5 !\n19/11/05 20:47:27 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_8_6 !\n19/11/05 20:47:27 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /10.128.0.24:34290\njava.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n\tat io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)\n\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\n('...trained ', 10, 'iterations in ', 4314.449198007584, ' seconds.')\nTop 20 ranked nodes:\n[('13455888', 0.001544724712983305), ('4695850', 0.0006710240718906528), ('5051368', 0.0005983856809747715), ('1184351', 0.000598207353646737), ('2437837', 0.0004624928928940761), ('6076759', 0.00045509400641448273), ('4196067', 0.0004423778888372456), ('13425865', 0.00044155351714348333), ('6172466', 0.00042240020018450447), ('1384888', 0.0004012895604073618), ('6113490', 0.0003957892477180534), ('14112583', 0.00039438472837547876), ('7902219', 0.0003700987847356982), ('10390714', 0.0003650264964328298), ('12836211', 0.00036199488631150274), ('6237129', 0.0003519555847625298), ('6416278', 0.0003486623564526654), ('13432150', 0.00033936510637418377), ('1516699', 0.0003329750028624433), ('7990491', 0.0003076090626586927)]\n19/11/05 21:10:33 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@30ad5d97{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n"
Tearing down cluster
