{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3 - Synonym Detection In Spark\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the last homework assignment you performed Naive Bayes to classify documents as 'ham' or 'spam.' In doing so, we relied on the implicit assumption that the list of words in a document can tell us something about the nature of that document's content. We'll rely on a similar intuition this week: the idea that, if we analyze a large enough corpus of text, the list of words that appear in small window before or after a vocabulary term can tell us something about that term's meaning.\n",
    "\n",
    "This will be your first assignment working in Spark. You'll perform Synonym Detection by repurposing an algorithm commonly used in Natural Language Processing to perform document similarity analysis. In doing so you'll also become familiar with important datatypes for efficiently processing sparse vectors and a number of set similarity metrics (e.g. Cosine, Jaccard, Dice). By the end of this homework you should be able to:  \n",
    "* ... __define__ the terms `one-hot encoding`, `co-occurrance matrix`, `stripe`, `inverted index`, `postings`, and `basis vocabulary` in the context of both synonym detection and document similarity analysis.\n",
    "* ... __explain__ the reasoning behind using a word stripe to compare word meanings.\n",
    "* ... __identify__ what makes set-similarity calculations computationally challenging.\n",
    "* ... __implement__ stateless algorithms in Spark to build stripes, inverted index and compute similarity metrics.\n",
    "* ... __apply__ appropriate metrics to assess the performance of your synonym detection algorithm. \n",
    "\n",
    "\n",
    "__`NOTE`__: your reading assignment for weeks 5 and 6 were fairly heavy and you may have glossed over the papers on dimension independent similarity metrics by [Zadeh et al](http://stanford.edu/~rezab/papers/disco.pdf) and pairwise document similarity by [Elsayed et al](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf). If you haven't already, this would be a good time to review those readings -- they are directly relevant to this assignment.\n",
    "\n",
    "__Please refer to the `README` for homework submission instructions and additional resources.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session (RUN THIS CELL AS IS)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw3_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '42093'),\n",
       " ('spark.app.name', 'hw3_notebook'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1569995065455'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'docker.w261')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark configuration Information (RUN THIS CELL AS IS)\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Spark Basics.\n",
    "In your readings and live session demos for weeks 4 and 5 you got a crash course in working with Spark. We also talked about how Spark RDDs fit into the broader picture of distributed algorithm design. The questions below cover key points from these discussions. Feel free to answer each one very briefly.\n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What is Spark? How  does it relate to Hadoop MapReduce?\n",
    "\n",
    "* __b) short response:__ In what ways does Spark follow the principles of statelessness (a.k.a. functional programming)? List at least one way in which it allows the programmer to depart from this principle. \n",
    "\n",
    "* __c) short response:__ In the context of Spark what is a 'DAG' and how do they relate to the difference between an 'action' and a 'transformation'? Why is it useful to pay attention to the DAG that underlies your Spark implementation?\n",
    "\n",
    "* __d) short response:__ Give a specific example of when we would want to `cache()` an RDD and explain why.\n",
    "\n",
    "* __e) environment:__ List your environment configuration in which you will be working for this assignment, including OS, Number of Cores, CPUs, Memory. Are you using Docker? Are you running in the cloud (what is your cloud Spark configuration)? (If you are working on a Mac, you can use the these commands to get the system configuration: `sw_vers` and `system_profiler SPHardwareDataType`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "> __a)__ Spark just like Hadoop is an open-source general-purpose distributed computing framework. It is the new generation of the MapReduce lineage. However, unlike Hadoop MapReduce, Spark doesn't require writing intermediate results to disk within a job. It also supports multi-stage iteratively jobs by reusing the data cached in memory which Hadoop doesn't support. Because of the potential time saved from writing to and rereading from disk, Spark is able to significantly outperform Hadoop.\n",
    "\n",
    "> __b)__ A Spark job is formed by a series of transformations and ends with an action call. The transformations are higher-order functions that take other functions as an input to perform a specific task. From each transfomation to a whole chain of transfomations and the culminating action (i.e., a Spark job) when given the same input, the same output will always be emitted. This is guaranteed by the stateless principle; that is, the logic of computations performed by a Spark job (or individual transformations and actions) is statless and doesn't change over time. \n",
    "\n",
    "> __c)__  \n",
    "> In the context of Spark, DAG stands for directed acyclic graph. It is a plan that shows the dependencies between the RDDs in a spark job. Since transformations are processes that return an RDD, usually, each unit on a DAG is some result of some transformation(s). A DAG is generated when an action is call therefore, each action marks an end of a DAG.  \n",
    "> It is important to pay attention to DAG because Spark optimizes our code under the hood; therefore, the code doesn't necessarily get executed sequentially line by line. Therefore, it is important to pay attention to DAG for designing, debugging and optimization purposes.\n",
    "\n",
    "> __d)__ When I need to use some result of a series of transformations for multiple actions, it would be a good time to cache the result so it doesn't have to be computed for every action it is used for. For example, after processing a series transformations and we would like to get the top and bottom twenty sorted by part of the result of the transformations. It might save time to cache the result before calling the first action to collect the top twenty results and the second action to collect the bottom twenty results.\n",
    "\n",
    "> __e)__\n",
    "> OS: Linux Ubuntu 18.04  \n",
    "> Number of Cores: 4  \n",
    "> Memory: 16GB  \n",
    "> Using Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Similarity Metrics\n",
    "As mentioned in the introduction to this assignment, an intuitive way to compare the meaning of two documents is to compare the list of words they contain. Given a vocabulary $V$ (feature set) we would represent each document as a vector of `1`-s and `0`-s based on whether or not it contains each word in $V$. These \"one-hot encoded\" vector representations allow us to use math to identify similar documents. However like many NLP tasks the high-dimensionality of the feature space is a challenge... especially when we start to scale up the size and number of documents we want to compare.\n",
    "\n",
    "In this question we'll look at a toy example of document similarity analysis. Consider these 3 'documents': \n",
    "```\n",
    "docA\tthe flight of a bumblebee\n",
    "docB\tthe length of a flight\n",
    "docC\tbuzzing bumblebee flight\n",
    "```\n",
    "These documents have a total of $7$ unique words: \n",
    ">`a, bumblebee, buzzing, flight, length, of, the`.     \n",
    "\n",
    "Given this vocabulary, the documents' vector representations are (note that one-hot encoded entries follow the order of the vocab list above):\n",
    "\n",
    "```\n",
    "docA\t[1,1,0,1,0,1,1]\n",
    "docB\t[1,0,0,1,1,1,1]\n",
    "docC\t[0,1,1,1,0,0,0]\n",
    "```  \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) short response:__ The cosine similarity between two vectors is $\\frac{A\\cdot B}{|A||B|}$. Explain what the the numerator and denominator of this calculation would represent in terms of word counts in documents A and B. \n",
    "\n",
    "* __b) short response:__ Explain how the Jaccard, Overlap and Dice metrics are similar/different to the calculation for cosine similarity. When would these metrics lead to different similarity rankings for a set of documents?\n",
    "\n",
    "* __c) short response:__ Calculate the cosine similarity for each pair of documents in our toy corpus. Please use markdown and $\\LaTeX$ to show your calcuations.  \n",
    "\n",
    "* __d) short response:__ According to your calculations in `part c` which pair of documents are most similar in meaning? Does this match your expecatation from reading the documents? If not, speculate about why we might have gotten this result.\n",
    "\n",
    "* __e) short response:__ In NLP common words like '`the`', '`of`', and '`a`' increase our feature space without adding a lot of signal about _semantic meaning_. Repeat your analysis from `part c` but this time ignore these three words in your calculations [__`TIP:`__ _to 'remove' stopwords just ignore the vector entries in columns corresponding to the words you wish to disregard_]. How do your results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Student Answers:\n",
    "> __a)__  \n",
    "> The numerator represents the number of unique words that shared by document A and B.  \n",
    "> The denominator represents the product of squared roots of numbers of unique words in document A and B respectively.\n",
    "\n",
    "> __b)__  \n",
    "> Similarity:\n",
    ">> They all use a range between 0 and 1 to represent the degree of similarity between two different sets or vectorized entities  \n",
    ">\n",
    "> Differences:\n",
    ">>  Overlap is the most different metric from the rest three purely because its definition. In the denominator, it only has the minimum of the unique words in document A and B. This makes it very partial to short documents. It might easily mark two documents to be highly similar just because one document is extremely short and it happens to share most of its words with a really long document even though the really important words that mark the themes of the documents have no overlap at all. Therefore, it's more likely for Overlap metric to rank short documents as having the highest similarity with some long documents.  \n",
    ">>  \n",
    ">> In contrast, Jaccard and Dice, are very harsh to short documents and will tend to rank them as very different from any long documents. It's obvious to see that any short documents will have very small overlap with other documents because of pure word counts. Therefore, when the similarity scores are calculated, they will have very small Jaccard and Dice values because they have a very small numerator and a decently large denominator (the sum of the lengths of a short document and a long document).  \n",
    ">>  \n",
    ">> Lastly, Cosine Similarity is the least biased out of all four in terms of document lengths since the document lengths' squared roots are used in the calculation instead.\n",
    "\n",
    "> __c)__  \n",
    "> $A = [1,1,0,1,0,1,1]; |A| = \\sqrt{5}$  \n",
    "> $B = [1,0,0,1,1,1,1]; |B| = \\sqrt{5}$  \n",
    "> $C = [0,1,1,1,0,0,0]; |A| = \\sqrt{3}$  \n",
    ">  \n",
    "> $cosim(A,B) = \\frac{A \\cdot B}{|A||B|} = \\frac{4}{\\sqrt{5} \\cdot \\sqrt{5}} = \\frac{4}{5} = 0.8 $  \n",
    "> $cosim(A,C) = \\frac{A \\cdot C}{|A||C|} = \\frac{2}{\\sqrt{5} \\cdot \\sqrt{3}} = \\frac{2}{\\sqrt{15}} \\approx 0.516 $  \n",
    "> $cosim(B,C) = \\frac{B \\cdot C}{|B||C|} = \\frac{1}{\\sqrt{5} \\cdot \\sqrt{3}} = \\frac{1}{\\sqrt{15}} \\approx 0.258 $\n",
    "\n",
    "\n",
    "> __d)__  \n",
    "> According to the calculation in `part c` documents A and B have the highest value of cosine similarity.  \n",
    "> It didn't quite match my expectation from looking at the documents because it seems like A and C should be more similar since they are both about the flight of bumblebees.\n",
    "> We got this result probably because A and B just happen to have more matching words from words such as, a, of, the, flight. Whereas, A and C only have two words matching like flight and bumblebee.\n",
    "\n",
    "> __e)__  \n",
    "> Documents A,B,C after the stop words are removed:  \n",
    "> $A = [1,0,1,0]; |A| = \\sqrt{2}$  \n",
    "> $B = [0,0,1,1]; |B| = \\sqrt{2}$  \n",
    "> $C = [1,1,1,0]; |A| = \\sqrt{3}$  \n",
    ">  \n",
    "> $sim(A,B) = \\frac{A \\cdot B}{|A||B|} = \\frac{1}{\\sqrt{2} \\cdot \\sqrt{2}} = \\frac{1}{2} = 0.5 $  \n",
    "> $sim(A,C) = \\frac{A \\cdot C}{|A||C|} = \\frac{2}{\\sqrt{2} \\cdot \\sqrt{3}} = \\frac{2}{\\sqrt{6}} \\approx 0.816 $  \n",
    "> $sim(B,C) = \\frac{B \\cdot C}{|B||C|} = \\frac{1}{\\sqrt{2} \\cdot \\sqrt{3}} = \\frac{1}{\\sqrt{6}} \\approx 0.408 $\n",
    ">  \n",
    "> The result after we removed the stopwords now reflect semantic meaning of the documents more. It shows that A and C are the most similar in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Synonym Detection Strategy\n",
    "\n",
    "In the Synonym Detection task we want to compare the meaning of words, not documents. For clarity, lets call the words whose meaning we want to compare `terms`. If only we had a 'meaning document' for each `term` then we could easily use the document similarity strategy from Question 2 to figure out which `terms` have similar meaning (i.e. are 'synonyms'). Of course in order for that to work we'd have to reasonably believe that the words in these 'meaning documents' really do reflect the meaning of the `term`. For a good analysis we'd also need these 'meaning documents' to be fairly long -- the one or two sentence dictionary definition of a term isn't going to provide enough signal to distinguish between thousands and thousands of `term` meanings.\n",
    "\n",
    "This is where the idea of co-occurrance comes in. Just like DocSim makes the assumption that words in a document tell us about the document's meaning, we're going to assume that the set of words that 'co-occur' within a small window around our term can tell us some thing about the meaning of that `term`. Remember that we're going to make this 'co-words' list (a.k.a. 'stripe') by looking at a large body of text. This stripe is our 'meaning document' in that it reflects all the kinds of situations in which our `term` gets used in real language. So another way to phrase our assumption is: we think `terms` that get used to complete lots of the same phrases probably have related meanings. This may seem like an odd assumption but computational linguists have found that it works surprisingly well in practice. Let's look at a toy example to build your intuition for why and how.\n",
    "\n",
    "Consider the opening line of Charles Dickens' _A Tale of Two Cities_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom it was the age of foolishness\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 10 unique words in this short 'corpus':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst', 'was', 'of', 'times', 'wisdom', 'age', 'foolishness', 'best', 'it', 'the']\n"
     ]
    }
   ],
   "source": [
    "words = list(set(re.findall(\"\\w+\", corpus.lower())))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of these 10 words, 4 are so common that they probably don't tell us very much about meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"it\", \"the\", \"was\", \"of\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll ignore these 'stop words' and we're left with a 6 word vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted([w for w in words if w not in stopwords])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in the tasks below is to asses, which of these six words are most related to each other in meaning -- based solely on this short two line body of text.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ Given this six word vocabulary, how many 'pairs' of words do we want to compare? More generally for a n-word vocabulary how many pairwise comparisons are there to make? \n",
    "\n",
    "* __b) code:__ In the space provided below, create a 'stripe' for each `term` in the vocabulary. This stripe should be the list of all other vocabulary words that occur within a __5 word window__ (two words on either side) of the `term`'s position in the original text.\n",
    "\n",
    "* __c) code + short response:__ Complete the provided code to turn your stripes into a 1-hot encoded co-occurrence matrix. For our 6 word vocabulary how many entries are in this matrix? How many entries are zeros? \n",
    "\n",
    "* __d) code:__ Complete the provided code to loop over all pairs and compute their cosine similarity. Please do not modify the existing code, just add your own in the spot marked.\n",
    "\n",
    "* __e) short response:__ Which pairs of words have the highest 'similarity' scores? Are these words 'synonyms' in the traditional sense? In what sense are their meanings 'similar'? Explain how our results are contingent on the input text. What would change if we had a much larger corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__  \n",
    "> For a six word vocabulary, the number of pairs of words we can have is:\n",
    "> ${6 \\choose 2} = \\frac{6 \\cdot 5}{1 \\cdot 2} = 15$  \n",
    "> Generally for an n-word vocabulary, we will have:  \n",
    "> ${n \\choose 2} = \\frac{n \\cdot (n-1)}{1 \\cdot 2} = \\frac{n \\cdot (n-1)}{2} pairs$\n",
    "\n",
    "\n",
    "> __c)__ There are a total of 36 entries. 8 entries out of the 36 are one and the rest of the 24 entries are zero. \n",
    "\n",
    "> __e)__  \n",
    "> The pairs `best-worst` and `foolishness-wisdom` have the highest similarity scores of 1.  \n",
    "> The are not synonmyms in the tratitional sense. In fact they are words of opposite meanings.  \n",
    "> Since our input is not really \"natural\" language. It's a poem, so it's more likely that synonyms and antonyms are going to be placed around similar (in this case, exact same words, again its poetry). This is how we got a similar score of one for those two pairs of words.  \n",
    "> In addition, we have zero as the similarity score for all other word pairs. This is probably not going to be the case given a much larger corpus where overlap is much more like to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS:\n",
      "It was the best of times, it was the worst of times, \n",
      "it was the age of wisdom it was the age of foolishness\n",
      "VOCAB:\n",
      "['age', 'best', 'foolishness', 'times', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "# for convenience, here are the corpus & vocab list again (RUN THIS CELL AS IS)\n",
    "print(\"CORPUS:\")\n",
    "print(corpus)\n",
    "print('VOCAB:')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "Image(filename=\"best-of-times.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - USE THE TEXT ABOVE TO COMPLETE EACH STRIPE\n",
    "stripes = {'age':['wisdom','foolishness'], # example\n",
    "           'best':['times'], # YOU FILL IN THE REST\n",
    "           'foolishness':['age'],\n",
    "           'times': ['best', 'worst'],\n",
    "           'wisdom':['age'],\n",
    "           'worst':['times']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - initializing an empty co-occurrence matrix (RUN THIS CELL AS IS)\n",
    "co_matrix = pd.DataFrame({term: [0]*len(vocab) for term in vocab}, index = vocab, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>times</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foolishness</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age  best  foolishness  times  wisdom  worst\n",
       "age            0     0            1      0       1      0\n",
       "best           0     0            0      1       0      0\n",
       "foolishness    1     0            0      0       0      0\n",
       "times          0     1            0      0       0      1\n",
       "wisdom         1     0            0      0       0      0\n",
       "worst          0     0            0      1       0      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part c - FILL IN THE MISSING LINE so that this cell 1-hot encodes the co-occurrence matrix\n",
    "for term, nbrs in stripes.items():\n",
    "    for nbr in nbrs:\n",
    "        pass\n",
    "        ############# YOUR CODE HERE #################\n",
    "        co_matrix.loc[term, nbr] = 1\n",
    "        ############# (END) YOUR CODE #################\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age-best: 0.0\n",
      "age-foolishness: 0.0\n",
      "age-times: 0.0\n",
      "age-wisdom: 0.0\n",
      "age-worst: 0.0\n",
      "best-foolishness: 0.0\n",
      "best-times: 0.0\n",
      "best-wisdom: 0.0\n",
      "best-worst: 1.0\n",
      "foolishness-times: 0.0\n",
      "foolishness-wisdom: 1.0\n",
      "foolishness-worst: 0.0\n",
      "times-wisdom: 0.0\n",
      "times-worst: 0.0\n",
      "wisdom-worst: 0.0\n"
     ]
    }
   ],
   "source": [
    "# part e - FILL IN THE MISSING LINES to compute the cosine similarity between each pair of terms\n",
    "for term1, term2 in itertools.combinations(vocab, 2):\n",
    "    # one hot-encoded vectors\n",
    "    v1 = co_matrix[term1]\n",
    "    v2 = co_matrix[term2]\n",
    "    \n",
    "    # cosine similarity\n",
    "    ############# YOUR CODE HERE #################\n",
    "    csim = np.sum(v1 * v2) / (np.sqrt(np.sum(v1)) * np.sqrt(np.sum(v2)))\n",
    "    ############# (END) YOUR CODE #################    \n",
    "    \n",
    "    print(f\"{term1}-{term2}: {csim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Pairs and Stripes at Scale\n",
    "\n",
    "As you read in the paper by Zadeh et al, the advantage of metrics like Cosine, Dice, Overlap and Jaccard is that they are dimension independent -- that is to say, if we implement them in a smart way the computational complexity of performing these computations is independent of the number of documents we want to compare (or in our case, the number of terms that are potential synonyms). One component of a 'smart implementation' involves thinking carefully both about how you define the \"basis vocabulary\" that forms your feature set (removing stopwords, etc). Another key idea is to use a data structure that facilitates distributed calculations. The DISCO implemetation further uses a sampling strategy, but that is beyond the scope of this assignment. \n",
    "\n",
    "In this question we'll take a closer look at the computational complexity of the synonym detection approach we took in question 3 and then revist the document similarity example as a way to explore a more efficient approach to parallelizing this analysis.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) short response:__ In question 3 you calculated the cosine similarity of pairs of words using the vector representation of their co-occurrences in a corpus. Imagine for now that you have unlimited memory on each of your nodes and describe a sequence of map & reduce steps that would start from a raw corpus and reproduce your strategy from Q3. For each step be sure to note what information would be stored in memory on your nodes and what information would need to be shuffled over the network (a bulleted list of steps with 1-2 sentences each is sufficient to answer this question).\n",
    "\n",
    "* __b) short response:__ In the asynch videos about \"Pairs and Stripes\" you were introduced to an alternative strategy. Explain two ways that using these data structures are more efficient than 1-hot encoded vectors when it comes to distributed similarity calculations [__`HINT:`__ _Consider memory constraints, amount of information being shuffled, amount of information being transfered over the network, and level of parallelization._]\n",
    "\n",
    "* __c) read provided code:__ The code below provides a streamined implementation of Document similarity analysis in Spark. Read through this code carefully. Once you are confident you understand how it works, answer the remaining questions. [__`TIP:`__ _to see the output of each transformation try commenting out the subsequent lines and adding an early `collect()` action_.]\n",
    "\n",
    "* __d) short response:__ The second mapper function, `splitWords`, emits 'postings'. The list of all 'postings' for a word is also refered to as an 'inverted index'. Define each of these terms based on your reading of the provided code.\n",
    "\n",
    "* __e) short response:__ The third mapper, `makeCompositeKeys`, loops over the inverted index to emit 'pairs' of what? Explain what information is included in the composite key created at this stage and why it makes sense to synchronize around that information in the context of performing document similarity calculations. In addition to the information included in these new keys, what other piece of information will we need to compute Jaccard or Cosine similarity?\n",
    "\n",
    "* __f) short response:__ Out of all the Spark transformations we make in this analysis, which are 'wide' transformations and which are 'narrow' transformations. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a)__  \n",
    "> First MapReduce:  \n",
    ">> __Map__: (we need to keep a look-up table of stopwords in memeory) make a pass through the documents and emit the a word when it is not a stopword  \n",
    ">> (partition and sorted based on words)  \n",
    ">> __Reduce__: when ever we encounter a new word, emit that word. As a result of this first MapReduce job, we should have our vocabulary  \n",
    "\n",
    "> Second MapReduce:  \n",
    ">> __Map__: (we need to keep a look-up table of our vocabulary) make another pass through the corpus and whenever we encounter a word in the vocabulary, take a look at the two adjacent words on each side. Emit a dicitonary with the word as the key and any of the four adjacent words that is in our vocab as values.  \n",
    ">> (partition and sorted based on the key)  \n",
    ">> __Reduce__: go through the words and combine the dictionaries with the same keys. Emit the stripes as the output of the reduce job.\n",
    "\n",
    "> Third MapReduce:  \n",
    ">> __Map__: (we need to keep a co-occurrence matrix generated from our vocabulary in memory) make a pass through our stripes and update the co-occuraence matrix. Emit the co-occurrence matrix  \n",
    ">> (no parition or sorting needed)  \n",
    ">> __Reduce__(use only one reducer): sum the co-occurrence matries from the mappers element-wise and emit the summed matrix\n",
    "\n",
    "> Forth MapReduce:  \n",
    ">> __Map__: (we need to keep a look-up table of our vocabulary in memory) make a pass through our vocaulary from disk and generate pairs with the vocabulary we kept in memory. Emit the pairs as key  \n",
    ">> (parition and sorting on the key)  \n",
    ">> __Reduce__: (we need to keep the co-occurrence matrix generated from the third MapReduce in memory). Make a pass through the pairs, whenever there is a new pair, we look up the information in the co-occurrence matrix and calculate the similarity. Emit the pairs of words and similarity score as output.\n",
    "\n",
    "> __b)__ The reason why pairs and stripes might be more efficient than the one-hot encoded vectors is mainly due to how condense we can represent a co-occurrence matrix. Especially when process natural language data and the volume of the text is large, we will have a very sparse matrix where most of the cells are zero. Therefore, the one-hot encoded vectors are not a very efficient way to represent the co-occurance matrix. When the co-occurance is being built, we need to shuffle the data and one-hot vectors just significantly increase the amount of data needs to be shuffled and transferred. Also, when we are calculating the similarity scores, there is also a lot more data we need to keep in memory if we choose one-hot vector representation than stripes or pairs representation. Also if we choose to use pairs or stripes to represent our co-occurance matrix, we could bypass the third MapReduce job in a) which creates a bottleneck when we could only use one reducer to sum the co-occurrence matries emitted by mappers.\n",
    "\n",
    "> __c)__ _read provided code before answering d-f_ \n",
    "\n",
    "> __d)__  \n",
    "> A posting is a key value pair with a certain term as key and the name of the document that contains the term and the length of the document as value. \n",
    "> A inverted index is just a collection of all postings for a term in our corpus. It contains information on the locations (in terms of document) of a given word in our corpus.\n",
    "\n",
    "> __e)__  \n",
    "> It emits key-value pairs where each pair means an instance of any two documents having the same term in them. Each pair has the documents names that contain the same word in our corpus as a composite key and one as value for each pair (which means 1 shared occurance).\n",
    "> It is important to synchronize and produce this composite key because we will have to aggregate the count by the key to get the number of shared word occurances to calculate document similarity.\n",
    "> We also have the lengths of each document recorded in the composite key since they are necessary for the similarity calculation.\n",
    "\n",
    "> __f)__  \n",
    "> The two reduceByKey transformation are wide because they will require data in different partitions to be suffled, so all of the data with the same key are together.\n",
    "> The rest of the map and flatMap transformation are narrow because when they are excuted, the work is done separately in each partition and no inter-partiton shuffling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test file: __`sample_docs.txt`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_docs.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_docs.txt\n",
    "docA\tbright blue butterfly forget\n",
    "docB\tbest forget bright sky\n",
    "docC\tblue sky bright sun\n",
    "docD\tunder butterfly sky hangs\n",
    "docE\tforget blue butterfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Document Similarity Analysis in Spark:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data - RUN THIS CELL AS IS\n",
    "data = sc.textFile(\"sample_docs.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def splitWords(pair):\n",
    "    \"\"\"Mapper 2: tokenize each document and emit postings.\"\"\"\n",
    "    doc, text = pair\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        yield (w, [(doc,len(words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def makeCompositeKey(inverted_index):\n",
    "    \"\"\"Mapper 3: loop over postings and yield pairs.\"\"\"\n",
    "    word, postings = inverted_index\n",
    "    # taking advantage of symmetry, output only (a,b), but not (b,a)\n",
    "    for subset in itertools.combinations(sorted(postings), 2):\n",
    "        yield (str(subset), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function - RUN THIS CELL AS IS\n",
    "def jaccard(line):\n",
    "    \"\"\"Mapper 4: compute similarity scores\"\"\"\n",
    "    (doc1, n1), (doc2, n2) = ast.literal_eval(line[0])\n",
    "    total = int(line[1])\n",
    "    jaccard = total / float(int(n1) + int(n2) - total)\n",
    "    yield doc1+\" - \"+doc2, jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('docA - docE', 0.75),\n",
       " ('docA - docB', 0.3333333333333333),\n",
       " ('docA - docC', 0.3333333333333333),\n",
       " ('docB - docC', 0.3333333333333333),\n",
       " ('docD - docE', 0.16666666666666666),\n",
       " ('docC - docE', 0.16666666666666666),\n",
       " ('docB - docE', 0.16666666666666666),\n",
       " ('docC - docD', 0.14285714285714285),\n",
       " ('docA - docD', 0.14285714285714285),\n",
       " ('docB - docD', 0.14285714285714285)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Job - RUN THIS CELL AS IS\n",
    "result = data.map(lambda line: line.split('\\t')) \\\n",
    "             .flatMap(splitWords) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(makeCompositeKey) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(jaccard) \\\n",
    "             .takeOrdered(10, key=lambda x: -x[1])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "Now that you are comfortable with similarity metrics we turn to the main task in this assignment: Synonym Detection. As you saw in Question 3 the ability of our algorithm to detect words with similar meanings is highly dependent on our input text. Specifically, we need a large enough corpus of natural language that we can expose our algorithm to a realistic range of contexts in which in any given word might get used. Ideally, these 'contexts' would also provide enough signal to distinguish between words with similar semantic roles but different meaning. Finding such a corpus will be easier to accomplish for some words than others.\n",
    "\n",
    "For the main task in this portion of the homework you will use data from Google's n-gram corpus. This data is particularly convenient for our task because Google has already done the first step for us: they windowed over a large subset of the web and extracted all 5-grams. If you are interested in learning more about this dataset the original source is: http://books.google.com/ngrams/, and a large subset is available [here from AWS](https://aws.amazon.com/datasets/google-books-ngrams/). \n",
    "\n",
    "For this assignment we have provided a subset of the 5-grams data consisting of 191 files of approximately 10MB each. These files are available on dropbox at: https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea\n",
    "... below we provide the code to download these files. Please only use the provided data so that we can ensure consistent results from student to student.\n",
    "\n",
    "Each row in our dataset represents one of these 5 grams in the format:\n",
    "> `(ngram) \\t (count) \\t (pages_count) \\t (books_count)`\n",
    "\n",
    "__DISCLAIMER__: In real life, we would calculate the stripes cooccurrence data from the raw text by windowing over the raw text and not from the 5-gram preprocessed data.  Calculating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some similar terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `data': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 2056M    0 2056M    0     0  36.0M      0 --:--:--  0:00:57 --:--:-- 39.1M\n"
     ]
    }
   ],
   "source": [
    "# download the zipped data folder - RUN THIS CELL AS IS\n",
    "!curl -L -o ngrams.zip https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      "replace data/googlebooks-eng-all-5gram-20090715-6-filtered.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# unzip the data files into the data directory - RUN THIS CELL AS IS\n",
    "!unzip -q ngrams.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global paths to full data folder and to the first file (which we'll use for testing)\n",
    "NGRAMS = PWD + '/data'\n",
    "F1_PATH = PWD + '/data/googlebooks-eng-all-5gram-20090715-0-filtered.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you develop your code you should use the following file to systems test each of your solutions before running it on the Google data. (Note: these are the 5-grams extracted from our two line Dickens corpus in Question 3... you should find that your Spark job results match the calculations we did \"by hand\").\n",
    "\n",
    "Test file: __`systems_test.txt`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting systems_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile systems_test.txt\n",
    "it was the best of\t1\t1\t1\n",
    "age of wisdom it was\t1\t1\t1\n",
    "best of times it was\t1\t1\t1\n",
    "it was the age of\t2\t1\t1\n",
    "it was the worst of\t1\t1\t1\n",
    "of times it was the\t2\t1\t1\n",
    "of wisdom it was the\t1\t1\t1\n",
    "the age of wisdom it\t1\t1\t1\n",
    "the best of times it\t1\t1\t1\n",
    "the worst of times it\t1\t1\t1\n",
    "times it was the age\t1\t1\t1\n",
    "times it was the worst\t1\t1\t1\n",
    "was the age of wisdom\t1\t1\t1\n",
    "was the best of times\t1\t1\t1\n",
    "was the age of foolishness\t1\t1\t1\n",
    "was the worst of times\t1\t1\t1\n",
    "wisdom it was the age\t1\t1\t1\n",
    "worst of times it was\t1\t1\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a Spark RDD for each of these files so that they're easy to access throughout the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark RDDs for each dataset\n",
    "testRDD = sc.textFile(\"systems_test.txt\") \n",
    "f1RDD = sc.textFile(F1_PATH)\n",
    "dataRDD = sc.textFile(NGRAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peak at what each of these RDDs looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was the best of\\t1\\t1\\t1',\n",
       " 'age of wisdom it was\\t1\\t1\\t1',\n",
       " 'best of times it was\\t1\\t1\\t1',\n",
       " 'it was the age of\\t2\\t1\\t1',\n",
       " 'it was the worst of\\t1\\t1\\t1',\n",
       " 'of times it was the\\t2\\t1\\t1',\n",
       " 'of wisdom it was the\\t1\\t1\\t1',\n",
       " 'the age of wisdom it\\t1\\t1\\t1',\n",
       " 'the best of times it\\t1\\t1\\t1',\n",
       " 'the worst of times it\\t1\\t1\\t1']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A BILL FOR ESTABLISHING RELIGIOUS\\t59\\t59\\t54',\n",
       " 'A Biography of General George\\t92\\t90\\t74',\n",
       " 'A Case Study in Government\\t102\\t102\\t78',\n",
       " 'A Case Study of Female\\t447\\t447\\t327',\n",
       " 'A Case Study of Limited\\t55\\t55\\t43',\n",
       " \"A Child's Christmas in Wales\\t1099\\t1061\\t866\",\n",
       " 'A Circumstantial Narrative of the\\t62\\t62\\t50',\n",
       " 'A City by the Sea\\t62\\t60\\t49',\n",
       " 'A Collection of Fairy Tales\\t123\\t117\\t80',\n",
       " 'A Collection of Forms of\\t116\\t103\\t82']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Bibliographic Guide to Writers\\t85\\t85\\t49',\n",
       " 'A Bibliography of John Dos\\t93\\t89\\t61',\n",
       " 'A CENTURY OF SPAIN AND\\t105\\t105\\t104',\n",
       " 'A Case Study of Chihuahua\\t140\\t140\\t108',\n",
       " 'A Case Study of Hong\\t273\\t270\\t211',\n",
       " 'A Caste Mobility Movement in\\t112\\t108\\t85',\n",
       " 'A Chapter in American Liberalism\\t103\\t90\\t45',\n",
       " 'A Chronicle of Louisbourg BY\\t58\\t58\\t58',\n",
       " 'A Collection of Historical Sketches\\t69\\t69\\t54',\n",
       " 'A Comparative Study of Male\\t132\\t125\\t101']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: N-gram EDA part 1 (words)\n",
    "\n",
    "Before starting our synonym-detection, let's get a sense for this data. As you saw in questions 3 and 4 the size of the vocabulary will impact the amount of computation we have to do. Write a Spark job that will accomplish the three tasks below as efficiently as possible. (No credit will be awarded for jobs that sort or subset after calling `collect()`-- use the framework to get the minimum information requested). As you develop your code, systems test each job on the provided file with Dickens ngrams, then on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) code:__ Write a Spark application to retrieve:\n",
    "  * The number of unique words that appear in the data. (i.e. size of the vocabulary) \n",
    "  * A list of the top 10 words & their counts.\n",
    "  * A list of the bottom 10 words & their counts.  \n",
    "  \n",
    "  __`NOTE  1:`__ _don't forget to lower case the ngrams before extracting words._  \n",
    "  __`NOTE  2:`__ _don't forget to take in to account the number of occurances of each ngram._  \n",
    "  __`NOTE  3:`__ _to make this code more reusable, the `EDA1` function code base uses a parameter 'n' to specify the number of top/bottom words to print (in this case we've requested 10)._\n",
    "\n",
    "\n",
    "* __b) short response:__ Given the vocab size you found in part a, how many potential synonym pairs could we form from this corpus? If each term's stripe were 1000 words long, how many individual 'postings' tuples would we need to shuffle in order to form the inverted indices? Show and briefly explain your calculations for each part of this question. [__`HINT:`__ see your work from q4 for a review of these concepts.]\n",
    "\n",
    "* __c) short response:__ What do you notice about the most frequent words, how useful will these top words be in synonym detection? Explain.\n",
    "\n",
    "* __d) short response:__ What do you notice/infer about the least frequent words, how reliable should we expect the detected 'synonyms' for the bottom words to be? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "\n",
    "> __b)__  \n",
    "> The vocab size was 269339. Therefore, the number of potential synonym pairs is the number of unique pairs that can be formed from choosing any two terms from the vocab. That is: ${269339 \\choose 2} = 36,271,613,791$  \n",
    "> For a stripe of 1000 words long, the number of postings that will be generated is 1000 per stripe. Moreover, we need to process 269339 stripes(one stripe per term). So the total number of postings we need to shuffle in order to form the inverted indices for the entire corpus is $269339 \\cdot 1000  = 269,339,000$\n",
    "\n",
    "> __c)__ The most frequent words are stopwords which doesn't indicate the semantics of the words they are in the same window with. Therefore, I think they not only will make the computation longer but might negatively impact the accuracy of our result. \n",
    "\n",
    "> __d)__ What I noticed right off the bat is that I've seen only one out of all the bottom words in my result. I think those are words that are not very commonly used. In terms of the reliability of our results for those words, I think it will be very questionable. Because of not enough data, their similarity with other words is in some degree determined by chance; that is if we run the same analysis on a different training corpus, we might have very different similarities for those words. Long story short, I don't expect the results for the least frequent words to be reliable just because we don't have enough information about them to make good inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - write your spark job here \n",
    "def EDA1(rdd, n):\n",
    "    total, top_n, bottom_n = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    def parse_lines(line):\n",
    "        \"\"\"parse lines and emit pairs of word and count\"\"\"\n",
    "        line_list = line.lower().split('\\t')\n",
    "        count = int(line_list[-3])\n",
    "        words = line_list[:-3][0]\n",
    "\n",
    "        return [(word,count) for word in words.split()]\n",
    "\n",
    "    word_countRDD = rdd.flatMap(parse_lines)\\\n",
    "                       .reduceByKey(lambda x,y: (x+y))\\\n",
    "                       .cache()\n",
    "\n",
    "    total = word_countRDD.map(lambda x: 1)\\\n",
    "                         .reduce(lambda x,y: x+y)\n",
    "\n",
    "    top_n = word_countRDD.takeOrdered(n, key=lambda x: -x[1])\n",
    "\n",
    "    bottom_n = word_countRDD.takeOrdered(n, key=lambda x: x[1])\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return total, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.524132490158081 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10\n",
      " ------ Top Words ------|--- Bottom Words ----\n",
      "         was         17 |    foolishness   1\n",
      "          of         17 |           best   4\n",
      "         the         17 |          worst   5\n",
      "          it         16 |         wisdom   5\n",
      "       times         10 |            age   8\n",
      "         age          8 |          times  10\n",
      "       worst          5 |             it  16\n",
      "      wisdom          5 |            was  17\n",
      "        best          4 |             of  17\n",
      " foolishness          1 |            the  17\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ------ Top Words ------|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>12} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for testRDD:\n",
    "\n",
    "    Vocabulary Size: 10\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     was         17 |    foolishness   1\n",
    "      of         17 |           best   4\n",
    "     the         17 |          worst   5\n",
    "      it         16 |         wisdom   5\n",
    "   times         10 |            age   8\n",
    "     age          8 |          times  10\n",
    "   worst          5 |             it  16\n",
    "  wisdom          5 |            was  17\n",
    "    best          4 |             of  17\n",
    "foolishness       1 |            the  17  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.9067256450653076 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run a single file, ie., a small sample (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(f1RDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 36353\n",
      " ------ Top Words ------|--- Bottom Words ----\n",
      "         the   27691943 |    stakeholder  40\n",
      "          of   18590950 |          kenny  40\n",
      "          to   11601757 |         barnes  40\n",
      "          in    7470912 |         arnall  40\n",
      "           a    6926743 |     buonaparte  40\n",
      "         and    6150529 |       puzzling  40\n",
      "        that    4077421 |             hd  40\n",
      "          is    4074864 |        corisca  40\n",
      "          be    3720812 |       cristina  40\n",
      "         was    2492074 |         durban  40\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ------ Top Words ------|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>12} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for f1RDD\n",
    "\n",
    "Vocabulary Size: 36353\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     the   27691943 |    stakeholder  40\n",
    "      of   18590950 |          kenny  40\n",
    "      to   11601757 |         barnes  40\n",
    "      in    7470912 |         arnall  40\n",
    "       a    6926743 |     buonaparte  40\n",
    "     and    6150529 |       puzzling  40\n",
    "    that    4077421 |             hd  40\n",
    "      is    4074864 |        corisca  40\n",
    "      be    3720812 |       cristina  40\n",
    "     was    2492074 |         durban  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 241.71937441825867 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - run full analysis (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "vocab_size, most_frequent, least_frequent = EDA1(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 269339\n",
      " ---- Top Words ----|--- Bottom Words ----\n",
      "     the 5490815394 |    viscerating  40\n",
      "      of 3698583299 |    unmurmuring  40\n",
      "      to 2227866570 |    scribbler's  40\n",
      "      in 1421312776 |      washermen  40\n",
      "       a 1361123022 |     foretastes  40\n",
      "     and 1149577477 |   schwetzingen  40\n",
      "    that  802921147 |       parcival  40\n",
      "      is  758328796 |      scholared  40\n",
      "      be  688707130 |            rll  40\n",
      "      as  492170314 |         mildes  40\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\" ---- Top Words ----|--- Bottom Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_frequent, least_frequent):\n",
    "    print(f\"{w1:>8} {c1:>10} |{w2:>15} {c2:>3}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for dataRDD:\n",
    "(bottom words might vary a little due to ties)\n",
    "\n",
    "Vocabulary Size: 269339\n",
    " ---- Top Words ----|--- Bottom Words ----\n",
    "     the 5490815394 |   schwetzingen  40\n",
    "      of 3698583299 |           cras  40\n",
    "      to 2227866570 |       parcival  40\n",
    "      in 1421312776 |          porti  40\n",
    "       a 1361123022 |    scribbler's  40\n",
    "     and 1149577477 |      washermen  40\n",
    "    that  802921147 |    viscerating  40\n",
    "      is  758328796 |         mildes  40\n",
    "      be  688707130 |      scholared  40\n",
    "      as  492170314 |       jaworski  40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: N-gram EDA part 2 (co-occurrences)\n",
    "\n",
    "The computational complexity of synonym analysis depends not only on the number of words, but also on the number of co-ocurrences each word has. In this question you'll take a closer look at that aspect of our data. As before, please test each job on small \"systems test\" (Dickens ngrams) file and on a single file from the Ngram dataset before running the full analysis.\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) code:__ Write a spark job that computes:\n",
    "  * the number of unique neighbors (i.e. 5-gram co-occuring words) for each word in the vocabulary. \n",
    "  * the top 10 words with the most \"neighbors\"\n",
    "  * the bottom 10 words with least \"neighbors\"\n",
    "  * a random sample of 1% of the words' neighbor counts  \n",
    "  __`NOTE:`__ for the last item, please return only the counts and not the words -- we'll go on to use these in a plotting function that expects a list of integers.\n",
    "\n",
    "\n",
    "* __b) short response:__ Use the provided code to plot a histogram of the sampled list from `a`. Comment on the distribution you observe. How will this distribution affect our synonym detection analysis?\n",
    "\n",
    "* __c) code + short response:__ Write a Spark Job to compare the top/bottom words from Q5 and from part a. Specifically, what % of the 1000 most/least neighbors words also appear in the list of 1000 most/least frequent words. [__`NOTE:`__ _technically these lists are short enough to comparing in memory on your local machine but please design your Spark job as if we were potentially comparing much larger lists._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "\n",
    "> __b)__  We see the distribution is very skewed. Most words have less than one hundred neighbors. A small portion of words have thousands of neighbor and those are more likely to be stopwords that are used very frequent but not important to our synonym detection. Since they have a large number of neighbors, when being tokenized, they generate long stripes and slow down the computation for no benefits to the accuracy of our results.\n",
    "\n",
    "> __c)__  \n",
    "> There is an overlap of 88% between the 1000 most neighbors words and 1000 most frequent words.  \n",
    "> There is an overlap of 1.9% between the 1000 least neighbors words and 1000 least frequent words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - spark job\n",
    "def EDA2(rdd,n):\n",
    "    top_n, bottom_n, sampled_counts = None, None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "\n",
    "    \n",
    "    def parse_lines(line):\n",
    "        \"\"\"parse lines and emit pairs of word pairs and count of one\"\"\"\n",
    "        line_list = line.lower().split('\\t')\n",
    "        count = int(line_list[-3])\n",
    "        words = line_list[:-3][0].split()\n",
    "    \n",
    "        words = set(words)\n",
    "        for subset in itertools.combinations(sorted(words), 2):\n",
    "            yield (subset, 1)\n",
    "\n",
    "    def separate_pairs(pair):\n",
    "        \"\"\"separate the pairs into two records for later aggregation \n",
    "        to sum the numbers of neighbors\"\"\"\n",
    "        return [(pair[0][0],1), (pair[0][1],1)]\n",
    "\n",
    "    pair_countRDD = rdd.flatMap(parse_lines)\\\n",
    "                       .combineByKey(lambda x: 1,\n",
    "                                     lambda x, y: 1,\n",
    "                                     lambda x, y: 1)\\\n",
    "                       .flatMap(separate_pairs)\\\n",
    "                       .reduceByKey(lambda x,y: x+y)\\\n",
    "                       .cache()\n",
    "    \n",
    "    \n",
    "    top_n = pair_countRDD.takeOrdered(n, key=lambda x: -x[1])\n",
    "\n",
    "    bottom_n = pair_countRDD.takeOrdered(n, key=lambda x: x[1])\n",
    "    \n",
    "    #get the total number of terms in the vocabulary\n",
    "    total = pair_countRDD.map(lambda x: 1)\\\n",
    "                         .reduce(lambda x,y: x+y)\n",
    "    \n",
    "    sampled_counts = pair_countRDD.map(lambda x: x[1])\\\n",
    "                                  .takeSample(False, int(0.01*total), 1)\n",
    "\n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return top_n, bottom_n, sampled_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.4410219192504883 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - systems test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(testRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "          of        9 |     foolishness    4\n",
      "         was        9 |            best    5\n",
      "         the        9 |           worst    5\n",
      "          it        8 |          wisdom    5\n",
      "         age        7 |             age    7\n",
      "       times        7 |           times    7\n",
      "        best        5 |              it    8\n",
      "       worst        5 |              of    9\n",
      "      wisdom        5 |             was    9\n",
      " foolishness        4 |             the    9\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for testRDD:\n",
    "\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         was        9 |     foolishness    4\n",
    "          of        9 |            best    5\n",
    "         the        9 |           worst    5\n",
    "          it        8 |          wisdom    5\n",
    "         age        7 |             age    7\n",
    "       times        7 |           times    7\n",
    "        best        5 |              it    8\n",
    "       worst        5 |             was    9\n",
    "      wisdom        5 |              of    9\n",
    " foolishness        4 |             the    9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.760148763656616 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - single file test (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(f1RDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the    25548 |              vo    1\n",
      "          of    22496 |           pizza    2\n",
      "         and    16489 |        premiers    2\n",
      "          to    14249 |        enclaves    2\n",
      "          in    13891 |   selectiveness    2\n",
      "           a    13045 |           trill    2\n",
      "        that     8011 |      noncleaved    2\n",
      "          is     7947 |             gem    2\n",
      "        with     7552 |            hoot    2\n",
      "          by     7400 |     palpitation    2\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for f1RDD:\n",
    "\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         the    25548 |              vo    1\n",
    "          of    22496 |      noncleaved    2\n",
    "         and    16489 |        premiers    2\n",
    "          to    14249 |        enclaves    2\n",
    "          in    13891 |   selectiveness    2\n",
    "           a    13045 |           trill    2\n",
    "        that     8011 |           pizza    2\n",
    "          is     7947 |            hoot    2\n",
    "        with     7552 |     palpitation    2\n",
    "          by     7400 |            twel    2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1163.6767432689667 seconds\n"
     ]
    }
   ],
   "source": [
    "# part a - full data (RUN THIS CELL AS IS)\n",
    "start = time.time()\n",
    "most_nbrs, least_nbrs, sample_counts = EDA2(dataRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Most Co-Words ---|--- Least Co-Words ----\n",
      "         the   164982 |          cococo    1\n",
      "          of   155708 |            inin    1\n",
      "         and   132814 |        charuhas    1\n",
      "          in   110615 |         ooooooo    1\n",
      "          to    94358 |           iiiii    1\n",
      "           a    89197 |          iiiiii    1\n",
      "          by    67266 |             cnj    1\n",
      "        with    65127 |            choh    1\n",
      "        that    61174 |             neg    1\n",
      "          as    60652 |      cococococo    1\n"
     ]
    }
   ],
   "source": [
    "# part a - display results (feel free to modify the formatting code if needed)\n",
    "print(\" --- Most Co-Words ---|--- Least Co-Words ----\")\n",
    "for (w1, c1), (w2, c2) in zip(most_nbrs, least_nbrs):\n",
    "    print(f\"{w1:>12} {c1:>8} |{w2:>16} {c2:>4}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Expected output for dataRDD: \n",
    "(bottom words might vary a little due to ties)\n",
    "\n",
    " --- Most Co-Words ---|--- Least Co-Words ----\n",
    "         the   164982 |          cococo    1\n",
    "          of   155708 |            inin    1\n",
    "         and   132814 |        charuhas    1\n",
    "          in   110615 |         ooooooo    1\n",
    "          to    94358 |           iiiii    1\n",
    "           a    89197 |          iiiiii    1\n",
    "          by    67266 |             cnj    1\n",
    "        with    65127 |            choh    1\n",
    "        that    61174 |             neg    1\n",
    "          as    60652 |      cococococo    1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`NOTE:`__ _before running the plotting code below, make sure that the variable_ `sample_counts` _points to the list generated in_ `part a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: we'll exclude the 6 words with more than 6000 nbrs in this 2687 count sample.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAE7CAYAAACc6mNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmcZFV58PHfSCvuAzQuzcMgJA4ENBFRWaJR3BBwGaOogLJJXPKCwUSNYlQMi2JeRQdF87og4AJOUATNKCIGB5JBUUIiiyYjIszD6OiAI0Iigv3+cU5D0VNVvUx31dyq3/fz6U9Xnbr33HNuV99zn3vOPXfB+Pg4kiRJkqRN3/36XQBJkiRJ0vQYwEmSJElSQxjASZIkSVJDGMBJkiRJUkMYwEmSJElSQxjASZIkSVJDGMBJQEScGBG/jIif9bssABExHhGP7dO2d4qIf4+I2yLir/pRhvkUEYdHxGX9Loek3omI90bEG+vrvSNi9RzkuXlE/DAiHrnxJRw8m1pbEhHvjojP9nH7m9R5xlyLiBsi4jn9LsewGOl3AdQ/EXED8Cjg7pbkHTPz5v6UqD8iYhHwJuAxmbm2zed7A/8CfDQzj2pJvwz4ZGae0ZuS9szfApdk5hM7LRARzwP+Dngi8L/AtcAHMvOCmWwoIg4C3pmZu7SkXQREm7SLM/PkGdVE0tCLiEcAhwJzelEsM38bEacDb6W0IUTE4cCngP9pWfSMzDx6LrfdEF3bkoi4BNgTWJyZN9W051Da1e17VchemOo8oy7zcOB44CXAVsDPgK8CJ2bmL2e4vR9R2tZl9f1TgcuAV0xKuxDYIjPvmlXF1Df2wOmFmfnQlp8NgreIGPRA/zHAuk4H1ep24NCI2L43RZobs/zbPQa4pkueBwD/BJwFbEu5CPAu4IWz2Na3gZ3rCdZEeZ8APHhS2l7AiplmHhGbzaJMkgbL4cDyzPyfqRachc8Dh0XE5i1pKye1q22DtyE4PnVtS6rbgXf2oCxzahZta9fzjIh4AHAx8DhgX+DhwJ8C64DdZ1HEFcAzWt4/Hfhhm7R/m2nwNgTnhI3gH0EbqEHKT4C/AI4DbgCeHhF7AqcAuwA/BY7JzEvqOjsAZwC7AZcDP6Jc1XlV/bzbupcAlwLPAv4EWAkcPHHFKSKeBvxDXfc2ysH+GsqVqZg4+ETESylXnHZtU6eFwIeB/YA7gE8A76nb/AqweUT8Bjg3Mw9vs1t+BZxX98cRbfJ/N/DYlvpO7MP7Z+ZdtY6XtdTxXygnFadSAp8fAS/LzBtast2/Dvl5OPBp4K2Z+fua/6uBtwCPBr4LvDYzf1o/GweOBt5I+R/foU15XwS8FwjgKuAvM/O6iPgW5QD/tIj4ELBbZv5Xy3oLKH/HEzLzky1Zfrv+EBH3A94OvAZ4EPB14A2ZuX5yOTLz5oi4ntKQfJHy/bkGuHFS2v2A79X8dwY+BuwKJHDsRM9fRJxBufL9mFqPJRHx73X/7U1pwC5sU59XAptTvpsHZ+bVk8sqqbH2A07v9OEUx5RRStv2DMpx+kJg78x8GkBmro6IWyk9Sd/uVogOx6dLgZOAl1OOQecBfz0RbEbEW4C/AcaBd1B69xZn5qoaNLZdt44c+SzwQUoP4d3A2zPz0zXfBwEnAgcAWwA/AJ4LnAt8PTM/3FLu/wTelZlfblOnWbUlLU4F3hwR/5CZq9rkPz5R35Z9uDoz39FSx1OBN9c6/iVwJ/AhYGvg/Zn5npYsHxgRXwD2B/4bOCIz/6PmvQ3lPOHpwG+AD2bmqfWzdwOPp4w4eVH9m7S2gRt7nnEosB3wzMz8TU1bC5zQkn/H72kbKyg9oBP+DHgftae4JW1Fzbtju93lnPAQynfooZR2tHVf7A58FNiR8p3/XGb+TYeyahbsgVM3zwB2Bp4XEQH8M+WfdSvKwfKLE70klKuQ36ccME8ADpvIZBrrAhxMCYweCTygLkNEbAd8jXJQfATlwHVVZl5BuTL13JY8XgV8pkNdPgwsBP6g1utQyoH7m5SD7c31SunhXfbHScBLI2KnLst0cyBwCKWh+0NKoPppyj65jnJgbPXnwJMpAcwS4NUAEfFiyoH2JZR9cilw9qR1XwzsQQl67yMidqzLv7Guvxz4SkQ8IDOfVfM7uu6PyQ3uTsAiSiPfyeH155mU/f1Q4CNdll9BaTCpvy+lBLutaZdn5p0RcX9KQ/gNynflDcDnJv1NDqb8rR5W8zmN0uiOUfbhq1uW3afmvyPlJOYVlO+VpMHxx5TgawPTOKacRuklejSlXTusTTbXUUYOTMfk49P7KMefXSlDPIMyooGI2JfSFj4XWAxMvr+o47rVoyntXgBHAqdFxJb1s/cDT6L08mxFOdn/PXAmpS2lluEJdf3lkyuykW3JhKQEOu/u8PlUHg08kHvr/ola/idRApR3RcQftCy/hDKCZCvKecuXI+L+NYD5CvAfNa9nA2+stwu0rnsupa34XJuybMx5xnMogfNv2nw2ne/pZN8GHhcRW9W6PRn4ArBFS9qfcu/IlsOZut1uPSfchRJMHgJsA4xSRuRMWAoszcyHU853lnUop2bJAE5fjohf1Z/JV9fenZm31yuBr6IMQVmemb/PzIsoPSL71yDrKZTer99m5grKgWZCx3Vblvl0Zv5X3dYySoMEpWfkm5l5dmb+LjPXZeZV9bN7GpqI2Ap4HuWAfB91mMorKFerbqu9XB+gHHimLTN/BvwjZYz6bHw6M39ce6K+Bvw4M79ZexD/iXI/Wav3ZeYtmXkj5WriQTX9dcB7M/O6uu57gF0j4jEt6763rttuyNArgH/OzIsy83eUhvxBlIP5VEbr7zVdlnklcEpmXl8bo2OBA7sMu/g29wZrf0Zp9C+dlDZxZXtPSsNycmbemZnfovTEHnRvdpyfmf9aeyt/B7yUcvX49tqzdmbLsr+jnEj9EbCg7tNudZPUPFtQRm+00/GYUtuOlwLHZeYdmXkt9z1+TLitbuOePFva1V/VESgTWo9Pv6X0ePx1PV7fRjmeH1iXfTml3bg6M2+nJcipowe6rQvl+HZ8bTuXU3qVdqon76+mjITJzLw7M/8tM38LnA8sjojFNY9DgC9k5p1t6r0xbUmr9wIvjIjHzXA9KHU8qW7/HMpF5KW1rb+GMqLjT1qW/35mnluXP4US/O1JOYd5RGYeX78H11OCwdb9uTIzv1zPY+7Tts7BecYo3dvV6bR996jnDTdS2s8nAP9dy/yvLWkPBL5TV5lOu916TngA8NXMXFG/N++kXACY8DvgsRGxdWb+JjMvn+Z+0DQ5hFIvrleH2rmp5fVjgJdFROt9TvenDAXcBri1NjATfkrpqZlq3QmtszLdQTlQUfP4cYfyfRa4LiIeSmnoLu1w8r01pVfvp5PKFx3y7eZ9wI/rVcmZ+nnL6/9p8/6h9138Pvv/p5T9DGV/Lo2ID7R8voBSn5+2WXeybVqWIzN/HxE3Mb39MdE7NUYZUjFl/vX1CPCoiHgn917dfU8d2rIC+FS9Mrwn8MrM/E1EjNW0p1EC2Im8b6onP635t5a9te6PqNuevC8ByMxvRcRHKFfZt4uI84A3Z+avO+4BSU1zK+VCTTvdjintjh/tjq0Powyzn3B51iGWbUw+Pj0Y+H4ZqAKUY/nEvXHbUEa2tJZruutCueeq9f6mibZ1a8rJ+wZta5aJWZYBr4qIv6cECAd0qMvGtCWt2/xFPQ4fT+nVmYl1mTkxEdtEUNWtbb1n/9fyrqbUYxzYJiJa/46bUS4mbrBuGxt7nrGO0q520vF7Wi+iXzuRmJkT9Z0Y3XIj99bjspa079TgayL/tu12S1pr/bfhvvvy9ohoHb1yJOXv+cOI+Anw95n51S710wwZwKmb8ZbXNwGfyczXTF6o9vxsGREPaQnitmtZv+O603ATHW7gzcyMiJWUoYaH0PnA/0vK1aDHcO9BbjvK0I0Zycx1dTz/CZM+up3SmE549EzzbmMR994Avh0wMcHMTZQrju2GcEwY7/LZzZQhRcA9V3IXMb398aO6/ZdSrrZ2yr+1N3A74C7g55n5euD1rQtn5vURcTPwWuDGliEkK2vaQyn3VU7kvSgi7tfSkG0HtA7Paa37L+q2F1Huf5tYvnX7pwKnRpkKfBnl3sLG3VQvqaP/pAw1vKLNZ92OKRPHj2259xizaMMs2JnS2zIdrcenX1ICjMdlZrvj75pJ22s9dk21bje/pAwr/0PKkMHJzqTcjnAZcEdmruyQz8a0JZP9X+B6yj3dre5gw7Z1Yx4Bcc/+rD2R21LqcRfwk8xc3GlFurerG3ue8U3gxEnnUa06fk9rb9vkC8BQArjXUYKxT9e0SynDgH/KfScG69huc+/QyNb6r6F87wGIiAdz7wgdMvO/Kb3Y96Pc7nFuRIx2qJtmwSGUmq7PUoY4PC8iNouIB0Z5ls62WSbP+B7w9xHxgCiTjrxwOutOY7ufA54TES+PiJGIGI2I1klKzqKM3f9jyg3cG6hX55YBJ0XEw2rA+Te1XLNxCmWIyM4taVdRburdrt7IfOws8271lojYMsr0w8dQxq9DGcZ57MRwk4hYGBEvm0G+y4DnR8Sz67j6N1GG8vzbVCtm5jhl370zIo6IiIdHxP0i4mkR8fG62NnAX0fEDrV39D2UITjdZrq6tObberXzspr2vZbhKt+hBMt/W+9b2JvyXTunQ3nvBr4EvDsiHlzH7bfen/mUiNij7ofbKSc1d7fLS1JjLee+s++16nhMaXP8+CPKfU33iNL9tRX3XmSatnoi/gngg/UCElFM3He1DDg8InapJ8jHzWDdqbZ7OnBKRGxT2+W9os6kWQO231OC0k73lU+Ub1ZtSZsy/apu728nfXQVcHAt4750/jtO15Mi4iV1aOAba3kvpwSOv46It0bEg+r2Hh8RT5lm+Tf2POMzlIujX4yIP6rt6mhEvD0i9meGbV+1gnJrxjMoQyehTFazA+Vet9YAbqbt9rnAC2rb/wBKb9s9MUVEvCoiHlG/axO9mratc8gATtOS5RktSyiTZ/yCcqB5C/d+hw6mTJpxC6WROWsG63bb7o2Ue+XeVPO+ivveLH4e5arReVNc2XkD5eB3PSUw+DxdZiWboky/psyKuVVL2kWUAOs/KUNe5mKowPk1r6sok8B8qm7rPMpQznMi4tfA1ZQbpKdb/h9RhjF+mHLV8IWUx0m0u8eh3frnUsb6v5py1e7nlAlqzq+LnE5pjFZQhln+L2X/d/Ntyo3ZrQ/YvrSm3dPI1DK+iFLfX1JmuTo0M39IZ0dTrk7+jDKb3KdbPns45SToVsoVyXV07lmU1ExnUe7XftDkD6ZxTDmaMjHFzyjHtbMpJ/0TDgbObBmKNlNvBVYBl9fj+Tcpk0WRmV+jDB//Vl3mW9NddxreTDmZv4LStr6P+7bJZ1EujHYMQDa2LWljKRue5B9T8/0V5T6tDWbCnKHzKe3XrZSROy+p9wjeXbezK6Xd+iVllsmFM8h71ucZ9fvzHMpIkYuAX1OCyq0pQx1n3PZlmThmLbCmBsgTwft3KW1fa6A9o3a73l94VK3jGsr+bO0Z3Re4Jsqsm0uBAzPzf6ezLzQ9C8bHu/UIS7MTk6bVn+dt/Rh4XZd7+SRJQywi3gOszcwPTblw93zeBzw6Myee/fYfwNOz+3NE50xMmlZ/HrdzKOXxNJ3u5ZPUR94Dp0aL8uy3cTa8MilJEgCZ+fbZrFeHTT6A0lv1FMrkDH9R8/wtZQbbgVKHa/4fSi+PpE2QAZwaK8rDsXcBDpk0M5MkSXPhYZRhk9tQhqN9gHuHiw+ceg/dlyjDMTd4LI+kTYNDKCVJkiSpIZzERJIkSZIawgBOkiRJkhpik7sHbs2aNRs9pnPhwoWsX79+LorTKNZ7eAxjncF6D5qxsbEF/S5Dk8xF+wiD+33qZhjrDNZ72FjvwdKtjRzIHriRkU0uLu0J6z08hrHOYL2luTCM36dhrDNY72FjvYfHQAZwkiRJkjSIDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhRvpdgPmw47su6vjZymN262FJJEnatNhGSlKz2QMnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDTHS7wJIkjTsImJn4Bhga+DizPxYn4skSdpEGcBJkjQPIuJ04AXA2sx8fEv6vsBSYDPgk5l5cmZeB7w+Iu4HfKIvBZYkNYJDKCVJmh9nAPu2JkTEZsBpwH7ALsBBEbFL/exFwGXAxb0tpiSpSeyBkyRpHmTmiojYflLy7sCqzLweICLOAZYA12bmBcAFEfHPwOcn57dw4UJGRua32R4dHZ3X/PtlZGRkYOvWjfUeLtZ7eBjASZLUOwHc1PJ+NbBHROwNvATYHFjebsX169fPe+HWrVs379voh9HR0YGtWzfWe7hY78EyNjbW8TMDOEmSemdBm7TxzLwEuKS3RZEkNZH3wEmS1DurgUUt77cFbu5TWSRJDWQPnCRJvXMFsDgidgASOBA4uL9FkiQ1iT1wkiTNg4g4G1gJ7BQRqyPiyMy8CzgauBC4DliWmdf0s5ySpGaxB06SpHmQmQd1SF9Oh4lKJEmaij1wkiRJktQQBnCSJEmS1BBTDqGMiEXAWcCjgd8DH8/MpRGxFfAFYHvgBuDlmXlrRCwAlgL7A3cAh2fmlTWvw4B31KxPzMwz57Y6kiRJkjS4ptMDdxfwpszcGdgTOCoidgHeBlycmYuBi+t7gP2AxfXntcDHAGrAdxywB7A7cFxEbDmHdZEkSZKkgTZlAJeZayZ60DLzNsqsWQEsASZ60M4EXlxfLwHOyszxzLwc2CIixoDnARdl5i2ZeStwEbDvnNZGkiRJkgbYjGahjIjtgScC3wEelZlroAR5EfHIicWAm1pWW13TOqXfx8KFCxkZmb/JMUdHR+ct734bGRkZ6Pp1Moz1HsY6g/WWJEmadqQUEQ8Fvgi8MTN/HbFB7DVhQZu08S7p97F+/frpFmlW1q1bN6/599Po6OhA16+TYaz3MNYZrPegGRsb63cRJElqnGnNQhkR96cEb5/LzC/V5J/XoZHU32tr+mpgUcvq2wI3d0mXJEmSJE3DlAFcnVXyU8B1mXlKy0cXAIfV14cB57ekHxoRCyJiT2B9HWp5IbBPRGxZJy/Zp6ZJkiRJkqZhOkMonwocAvwgIq6qaW8HTgaWRcSRwI3Ay+pnyymPEFhFeYzAEQCZeUtEnABcUZc7PjNvmZNaSJIkSdIQmDKAy8zLaH//GsCz2yw/DhzVIa/TgdNnUkBJkiRJUjGte+AkSZIkSf1nACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ1hACdJkiRJDWEAJ0mSJEkNYQAnSZIkSQ0xMtUCEXE68AJgbWY+vqa9G3gN8Iu62Nszc3n97FjgSOBu4K8y88Kavi+wFNgM+GRmnjy3VZEkSZKkwTZlAAecAXwEOGtS+gcz8/2tCRGxC3Ag8DhgG+CbEbFj/fg04LnAauCKiLggM6/diLJLkiRJ0lCZcghlZq4AbplmfkuAczLzt5n5E2AVsHv9WZWZ12fmncA5dVlJkiRJ0jRNpweuk6Mj4lDge8CbMvNWIIDLW5ZZXdMAbpqUvke7TBcuXMjIyMYUq7vR0dF5y7vfRkZGBrp+nQxjvYexzmC9JUmSZhspfQw4ARivvz8AvBpY0GbZcdr39I23y3j9+vWzLNL0rFu3bl7z76fR0dGBrl8nw1jvYawzWO9BMzY21u8iaJK9ll7Z8bOVx+zWw5JIkjqZVQCXmT+feB0RnwC+Wt+uBha1LLotcHN93SldkiRJkjQNswrgImIsM9fUt38OXF1fXwB8PiJOoUxishj4LqVnbnFE7AAkZaKTgzem4JIkDYqIeDHwfOCRwGmZ+Y0+F0mStImazmMEzgb2BraOiNXAccDeEbErZRjkDcDrADLzmohYBlwL3AUclZl313yOBi6kPEbg9My8Zs5rI0nSJqLdY3hq+gaP1cnMLwNfjogtgfcDBnCSpLamDOAy86A2yZ/qsvxJwElt0pcDy2dUOkmSmusMJj2GJyI2o/tjdd5RP5ckqa35m+5RkqQhlpkrImL7Scn3PFYHICLOAZZExHXAycDXMrPtTCLzPUvzVJo8E+qwzuRqvYeL9R4eBnCSJPVO0P6xOm8AngMsjIjHZuY/Tl5xvmdpnkqTZ0Id1Jlcp2K9h4v1HizdZmo2gJMkqXfaPm4nM08FTu11YSRJzdPu+WySJGl+dHvcjiRJU7IHTpKk3rkCH6sjSdoI9sBJkjQP6mN4VgI7RcTqiDgyM+8CJh6rcx2wzMfqSJJmwh44SZLmQYfH8PhYHUnSRrEHTpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGmKk3wWQJEmbvr2WXtnxs5XH7NbDkkjScLMHTpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIaYqTfBZAkSc2219Ir26avPGa3HpdEkgafPXCSJEmS1BAGcJIkSZLUEAZwkiRJktQQU94DFxGnAy8A1mbm42vaVsAXgO2BG4CXZ+atEbEAWArsD9wBHJ6ZV9Z1DgPeUbM9MTPPnNuqSJIkSdJgm84kJmcAHwHOakl7G3BxZp4cEW+r798K7Acsrj97AB8D9qgB33HAk4Fx4PsRcUFm3jpXFZEkSZuWTpObgBOcSNJsTTmEMjNXALdMSl4CTPSgnQm8uCX9rMwcz8zLgS0iYgx4HnBRZt5Sg7aLgH3nogKSJEmSNCxm+xiBR2XmGoDMXBMRj6zpAdzUstzqmtYpfQMLFy5kZGT+nm4wOjo6b3n328jIyEDXr5NhrPcw1hmstyRJ0lxHSgvapI13Sd/A+vXr57RAk61bt25e8++n0dHRga5fJ8NY72GsM1jvQTM2NtbvIkiS1DiznYXy53VoJPX32pq+GljUsty2wM1d0iVJkiRJ0zTbAO4C4LD6+jDg/Jb0QyNiQUTsCayvQy0vBPaJiC0jYktgn5omSZIkSZqm6TxG4Gxgb2DriFhNmU3yZGBZRBwJ3Ai8rC6+nPIIgVWUxwgcAZCZt0TECcAVdbnjM3PyxCiSJGlIOEOlJM3OlAFcZh7U4aNnt1l2HDiqQz6nA6fPqHSSJEmSpHvMdgilJEmSJKnHDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSGmfJC3JEnSpmKvpVd2/GzlMbv1sCSS1B/2wEmSJElSQxjASZIkSVJDGMBJkiRJUkMYwEmSJElSQxjASZIkSVJDGMBJkiRJUkP4GAFJkvosIv4A+DtgYWYe0O/ySJI2XfbASZI0DyLi9IhYGxFXT0rfNyJ+FBGrIuJtAJl5fWYe2Z+SSpKaxABOkqT5cQawb2tCRGwGnAbsB+wCHBQRu/S+aJKkpjKAkyRpHmTmCuCWScm7A6tqj9udwDnAkp4XTpLUWN4DJ0lS7wRwU8v71cAeETEKnAQ8MSKOzcz3Tl5x4cKFjIwMR7M9Ojo6p+uNjIzMOs8ms97DxXoPj+FoCSRJ2jQsaJM2npnrgNd3W3H9+vXzU6JN0Lp16+Z0vdHR0Vnn2WTWe7hY78EyNjbW8TOHUEqS1DurgUUt77cFbu5TWSRJDWQPnCRJvXMFsDgidgASOBA4uL9F2vTstfTKfhdBkjZZ9sBJkjQPIuJsYCWwU0SsjogjM/Mu4GjgQuA6YFlmXtPPckqSmsUeOEmS5kFmHtQhfTmwvMfFkSQNCHvgJEmSJKkhDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSGchVKSJA21bs+dW3nMbj0siSRNzR44SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIJzGRJEnqoNsEJ904+Ymk+WIPnCRJkiQ1hAGcJEmSJDWEAZwkSZIkNYQBnCRJkiQ1hAGcJEmSJDWEs1BKkqSBMNsZI7WhbvvSGTal/rIHTpIkSZIawgBOkiRJkhrCAE6SJEmSGsIATpIkSZIaYqMmMYmIG4DbgLuBuzLzyRGxFfAFYHvgBuDlmXlrRCwAlgL7A3cAh2emdxtLkiRJ0jTNRQ/cMzNz18x8cn3/NuDizFwMXFxlH9FAAAAKKUlEQVTfA+wHLK4/rwU+NgfbliRJkqShMR+PEVgC7F1fnwlcAry1pp+VmePA5RGxRUSMZeaaeSiDJEnSJmk2U/TPx7T+s33sQq8fMeAjDaT72tgAbhz4RkSMA/8vMz8OPGoiKMvMNRHxyLpsADe1rLu6pt0ngFu4cCEjI/P3eLrR0dF5y7vfRkZGBrp+nQxjvYexzmC9JUmSNjZSempm3lyDtIsi4oddll3QJm18csL69es3skjdrVu3bl7z76fR0dGBrl8nw1jvYawzWO9BMzY21u8iSJLUOBt1D1xm3lx/rwXOA3YHfh4RYwD199q6+GpgUcvq2wI3b8z2JUmSJGmYzDqAi4iHRMTDJl4D+wBXAxcAh9XFDgPOr68vAA6NiAURsSew3vvfJEmSJGn6NmYI5aOA8yJiIp/PZ+bXI+IKYFlEHAncCLysLr+c8giBVZTHCByxEduWJEmSpKEz6wAuM68HntAmfR3w7Dbp48BRs92eJEmSJA27uXgOnCRJkiSpBwzgJEmSJKkh5u+Ba5IkSdKAmO2Dz3v9sHEffD747IGTJEmSpIYwgJMkSZKkhjCAkyRJkqSGMICTJEmSpIYwgJMkSZKkhjCAkyRJkqSGMICTJEmSpIYwgJMkSZKkhjCAkyRJkqSGMICTJEmSpIYwgJMkSZKkhjCAkyRJkqSGGOl3ASRJkgbNXkuvHMhtTWW2ZVl5zG5zvr3Z5jnI3F+DwR44SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIAzhJkiRJaggDOEmSJElqCAM4SZIkSWoIAzhJkiRJaggDOEmSJElqiJF+F0CSpGEXEQ8BPgrcCVySmZ/rc5EkSZsoe+AkSZoHEXF6RKyNiKsnpe8bET+KiFUR8baa/BLg3Mx8DfCinhdWktQYQ9cDt9fSKzt+tvKY3XpYEknSgDsD+Ahw1kRCRGwGnAY8F1gNXBERFwDbAj+oi93d22JKkppk6AI4SZJ6ITNXRMT2k5J3B1Zl5vUAEXEOsIQSzG0LXEWH0TELFy5kZMRme9B1u9A8yEZHR+c8z0778r+Of+6cb6ubbnXb8V0XdfxspuUcGRnZqP04m3LOdl/OR73nMs8J3fLsZr6/Y7YEkiT1TgA3tbxfDewBnAp8JCKeD3yl3Yrr16+f/9JJfbJu3bqB3NbGbG+m642Ojm5U3Waz7nzsy/mod1P+5q3GxsY6fmYAJ0lS7yxokzaembcDR/S6MJKk5nESE0mSemc1sKjl/bbAzX0qiySpgeyBkySpd64AFkfEDkACBwIH97dIkqQmsQdOkqR5EBFnAyuBnSJidUQcmZl3AUcDFwLXAcsy85p+llOS1Cz2wEmSNA8y86AO6cuB5T0ujiRpQNgDJ0mSJEkNYQ9cCx/yLUmSJGlT1vMALiL2BZYCmwGfzMyTe12G2TC4kyRJktRvPQ3gImIz4DTguZSplK+IiAsy89pelmOudQvuujHwkyRJkjQTve6B2x1YlZnXA0TEOcASoNEB3GwZ+EmSJEmaiQXj4+M921hEHADsm5l/Ud8fAuyRmUf3rBCSJEmS1FC9noVyQZu03kWQkiRJktRgvQ7gVgOLWt5vC9zc4zJIkiRJUiP1+h64K4DFEbEDkMCBwME9LoMkSZIkNVJPA7jMvCsijgYupDxG4PTMvGYut9HUxxS0ExGnAy8A1mbm42vaVsAXgO2BG4CXZ+atEbGAUu/9gTuAwzPzyrrOYcA7arYnZuaZvazHTEXEIuAs4NHA74GPZ+bSQa97RDwQWAFsTvnfPDczj6sXPM4BtgKuBA7JzDsjYnPKfnoSsA54RWbeUPM6FjgSuBv4q8y8sNf1mYk6Q+33gMzMFwxJnW8AbqOU967MfPKgf8fVX4PUPsJwtpG2j8PXPoJtJLaRG+j1EEoyc3lm7piZf5iZJ81l3i2PKdgP2AU4KCJ2mctt9NgZwL6T0t4GXJyZi4GL63sodV5cf14LfAzuacyOA/agzAJ6XERsOe8l3zh3AW/KzJ2BPYGj6t9x0Ov+W+BZmfkEYFdg34jYE3gf8MFa71spB2Dq71sz87HAB+ty1H11IPA4yvfno/V/Y1N2DHBdy/thqDPAMzNz18x8cn0/6N9x9ckAto8wnG2k7ePwtY9gG2kbOUnPA7h5ds9jCjLzTsrViSV9LtOsZeYK4JZJyUuAiasHZwIvbkk/KzPHM/NyYIuIGAOeB1yUmbdk5q3ARWzY4G1SMnPNxJWTzLyNctAKBrzutfy/qW/vX3/GgWcB59b0yfWe2B/nAs+uV6GWAOdk5m8z8yfAKsr/xiYpIrYFng98sr5fwIDXuYuB/o6rrwaqfYThbCNtH4Ehah/BNnKSgf6ez8SgBXAB3NTyfnVNGySPysw1UA7kwCNreqe6N3qfRMT2wBOB7zAEdY+IzSLiKmAt5UDzY+BXmXlXXaS1DvfUr36+HhilefX+EPC3lOFAUOow6HWGcvLxjYj4fkS8tqYN/HdcfTMs35Wh+R+yfRyK9hFsI20j2xi0AG6YH1PQqe6N3ScR8VDgi8AbM/PXXRYdmLpn5t2ZuStlhtbdgZ3bLDZRh8bXOyIm7l/5fktyt/I3vs4tnpqZu1GGfhwVEU/vsuwg1Vv9MezflYH6H7J9HPz2EWwjbSM7G7QAbhgeU/Dz2i1M/b22pneqeyP3SUTcn9I4fS4zv1STh6LuAJn5K+ASyj0OW0TExIRDrXW4p37184WU4URNqvdTgRfVm5XPoQwL+RCDXWcAMvPm+nstcB7lhGRovuPquWH5rgz8/5Dt49C0j2AbaRvZwaAFcPc8piAiHkC5YfOCPpdprl0AHFZfHwac35J+aEQsqDf2rq/dyxcC+0TElvXGzX1q2iarjtf+FHBdZp7S8tFA1z0iHhERW9TXDwKeQ7m/4V+AA+pik+s9sT8OAL6VmeM1/cCI2LzOVLUY+G5vajEzmXlsZm6bmdtT/l+/lZmvZIDrDBARD4mIh028pnw3r2bAv+Pqq2FoH2HA/4dsH4enfQTbyInX2EZuoNfPgZtX2YPHFPRSRJwN7A1sHRGrKTPpnAwsi4gjgRuBl9XFl1OmT11FmUL1CIDMvCUiTqA03gDHZ+bkm743NU8FDgF+UMe7A7ydwa/7GHBmnRnqfsCyzPxqRFwLnBMRJwL/Tmm8qb8/ExGrKFfYDgTIzGsiYhlwLWXGsqMy8+4e12VjvZXBrvOjgPMiAspx+POZ+fWIuILB/o6rTwatfYShbSNtH20fwTZyUL/n07ZgfHwghoJKkiRJ0sAbtCGUkiRJkjSwDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSEM4CRJkiSpIQzgJEmSJKkhDOAkSZIkqSH+P1QnGsMgSu0XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# part b - plot histogram (RUN THIS CELL AS IS - feel free to modify format)\n",
    "\n",
    "# removing extreme upper tail for a better visual\n",
    "counts = np.array(sample_counts)[np.array(sample_counts) < 6000]\n",
    "t = sum(np.array(sample_counts) > 6000)\n",
    "n = len(counts)\n",
    "print(\"NOTE: we'll exclude the %s words with more than 6000 nbrs in this %s count sample.\" % (t,n))\n",
    "\n",
    "# set up figure\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "# plot regular hist\n",
    "ax1.hist(counts, bins=50)\n",
    "ax1.set_title('Freqency of Number of Co-Words', color='0.1')\n",
    "ax1.set_facecolor('0.9')\n",
    "ax1.tick_params(axis='both', colors='0.1')\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot log scale hist\n",
    "ax2.hist(counts, bins=50)\n",
    "ax2.set_title('(log)Freqency of Number of Co-Words', color='0.1')\n",
    "ax2.set_facecolor('0.9')\n",
    "ax2.tick_params(axis='both', colors='0.1')\n",
    "ax2.grid(True)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - spark job\n",
    "def compareRankings(rdd1, rdd2):\n",
    "    percent_overlap = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    \n",
    "    overlap = rdd1.join(rdd2).count()\n",
    "    percent_overlap = float(overlap / 1000) * 100\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return percent_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 1000 words with most neighbors, 88.0 percent are also in the list of 1000 most frequent words.\n",
      "Of the 1000 words with least neighbors, 1.9 percent are also in the list of 1000 least frequent words.\n"
     ]
    }
   ],
   "source": [
    "# part c - get lists for comparison (RUN THIS CELL AS IS...)\n",
    "# (... then change 'testRDD' to 'f1RDD'/'dataRDD' when ready)\n",
    "total, topWords, bottomWords = EDA1(dataRDD, 1000)\n",
    "topNbrs, bottomNbrs, sample_counts = EDA2(dataRDD, 1000)\n",
    "twRDD = sc.parallelize(topWords)\n",
    "bwRDD = sc.parallelize(bottomWords)\n",
    "tnRDD = sc.parallelize(topNbrs)\n",
    "bnRDD = sc.parallelize(bottomNbrs)\n",
    "top_overlap = compareRankings(tnRDD, twRDD)\n",
    "bottom_overlap = compareRankings(bnRDD,bwRDD)\n",
    "print(f\"Of the 1000 words with most neighbors, {top_overlap} percent are also in the list of 1000 most frequent words.\")\n",
    "print(f\"Of the 1000 words with least neighbors, {bottom_overlap} percent are also in the list of 1000 least frequent words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Basis Vocabulary & Stripes\n",
    "\n",
    "Every word that appears in our data is a potential feature for our synonym detection analysis. However as we've discussed, some are likely to be more useful than others. In this question, you'll choose a judicious subset of these words to form our 'basis vocabulary' (i.e. feature set). Practically speaking, this means that when we build our stripes, we are only going to keep track of when a term co-occurs with one of these basis words. \n",
    "\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ Suppose we were deciding between two different basis vocabularies: the 1000 most frequent words or the 1000 least frequent words. How would this choice impact the quality of the synonyms we are able to detect? How does this choice relate to the ideas of 'overfitting' or 'underfitting' a training set?\n",
    "\n",
    "* __b) short response:__ If we had a much larger dataset, computing the full ordered list of words would be extremely expensive. If we need to none-the-less get an estimate of word frequency in order to decide on a basis vocabulary (feature set), what alternative strategy could we take?\n",
    "\n",
    "* __c) code:__ Write a spark job that does the following:\n",
    "  * tokenizes, removes stopwords and computes a word count on the ngram data\n",
    "  * subsets the top 10,000 words (these are the terms we'll consider as potential synonyms)\n",
    "  * subsets words 9,000-9,999 (this will be our 1,000 word basis vocabulary)    \n",
    "  (to put it another way - of the top 10,000 words, the bottom 1,000 form the basis vocabulary)\n",
    "  * saves the full 10K word list and the 1K basis vocabulary to file for use in `d`.  \n",
    "  \n",
    "  __NOTE:__ _to ensure consistency in results please use only the provided list of stopwords._  \n",
    "  __NOTE:__ _as always, be sure to test your code on small files as you develop it._  \n",
    "\n",
    "* __d) code:__ Write a spark job that builds co-occurrence stripes for the top 10K words in the ngram data using the basis vocabulary you developed in `part c`. This job/function, unlike others so far, should return an RDD (which we will then use in q8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ \n",
    "> It is not too difficult to imagine that 1000 most frequent words are probably going to be similar between datasets that are aquired the same way. On the other hand, the least common words might vary more from dataset to dataset.   \n",
    "> When using the 1000 least frequent words, since those words are not very common, we are making predictions based on very few data points. They will make our results inaccurate, unreliable and sensitive to the local information of the training data. This is a case of overfitting.\n",
    "> Most of the 1000 most frequent words are probably just words that are commonly used but not semantically meaningful enough to use its co-ocurrance with others to determine term similarity. Though they remain stable across different possible training sets, they don't provide useful enough information to make predictions. Therefore, in this case, our accuracy might still suffer and we might have underfitting issues.\n",
    "\n",
    "> __b)__  \n",
    "> We can remove stopwords before trying to compute for the basis.\n",
    "> We can also try randomly sampling the tokenized data within partitions. Then do the subsequent work(reduceByKey and sort). This will probably give us a decent estimate of the word frequency without having to pay the full price of shuffling the entire dataset around. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - provided stopwords (RUN THIS CELL AS IS)\n",
    "STOPWORDS =  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 'should', 'can', 'now', 'will', 'just', \n",
    "              'would', 'could', 'may', 'must', 'one', 'much', \"it's\",\n",
    "              \"can't\", \"won't\", \"don't\", \"shouldn't\", \"hasn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - spark job\n",
    "def get_vocab(rdd, n_total, n_basis):\n",
    "    vocab, basis = None, None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    #conver STOPWORDS to set for fast lookup\n",
    "    stopwrods = set(STOPWORDS)\n",
    "\n",
    "    def parse_lines(line):\n",
    "        \"\"\"parse the lines and \n",
    "        emit tokens of word count pairs for non-stopwords\"\"\"\n",
    "        line_list = line.lower().split('\\t')\n",
    "        count = int(line_list[-3])\n",
    "        words = line_list[:-3][0]\n",
    "\n",
    "        return [(word,count) for word in words.split() if word not in stopwrods]\n",
    "\n",
    "    word_countRDD = rdd.flatMap(parse_lines)\\\n",
    "                       .reduceByKey(lambda x,y: (x+y))\\\n",
    "                       .cache()\n",
    "    \n",
    "    top_n = word_countRDD.takeOrdered(n_total, key=lambda x: -x[1])\n",
    "    \n",
    "    # get the vocab and basis\n",
    "    vocab = [word for word,count in top_n]\n",
    "    basis = vocab[-n_basis:]\n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return vocab, basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - run your job (replace 'testRDD' with 'f1RDD'/'dataRDD' when ready)\n",
    "VOCAB, BASIS = get_vocab(dataRDD, 10000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - save to file (RUN THIS CELL AS IS)\n",
    "with open(\"vocabulary.txt\", \"w\") as file:\n",
    "    file.write(str(VOCAB))\n",
    "with open(\"basis.txt\", \"w\") as file:\n",
    "    file.write(str(BASIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - spark job\n",
    "def buildStripes(rdd, vocab, basis):\n",
    "    stripesRDD = None\n",
    "    ############# YOUR CODE HERE ###############\n",
    "    vocab = set(vocab)\n",
    "    basis = set(basis)\n",
    "    \n",
    "    def parse_lines(line):\n",
    "        \"\"\"parse the lines and filter out words not in the vocab\"\"\"\n",
    "        line_list = line.lower().split('\\t')\n",
    "        count = int(line_list[-3])\n",
    "        words = line_list[:-3][0].split()\n",
    "        filtered_words = [word for word in words if word in vocab]\n",
    "        return (filtered_words, count)\n",
    "\n",
    "    def generate_stripes(words, count):\n",
    "        \"\"\"generate stripes from lines\"\"\"\n",
    "        for i in range(len(words)):\n",
    "            stripe = set()\n",
    "            for j in range(len(words)):\n",
    "                if words[i] != words[j] and words[j] in basis:\n",
    "                    stripe.add(words[j])\n",
    "            yield (words[i], stripe)\n",
    "\n",
    "    stripesRDD = rdd.map(parse_lines)\\\n",
    "                    .flatMap(lambda x: generate_stripes(*x))\\\n",
    "                    .combineByKey(lambda x: x,\n",
    "                                  lambda x, y: x.union(y),\n",
    "                                  lambda x, y: x.union(y))\n",
    "    \n",
    "    \n",
    "    ############# (END) YOUR CODE ##############\n",
    "    return stripesRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[216] at RDD at PythonRDD.scala:49\n",
      "Wall time: 0.00830984115600586 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run your systems test (RUN THIS CELL AS IS)\n",
    "VOCAB, BASIS = get_vocab(testRDD, 10, 10)\n",
    "testStripesRDD = buildStripes(testRDD, VOCAB, BASIS)\n",
    "start = time.time()\n",
    "print(testStripesRDD)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.6600899696350098 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run your single file test (RUN THIS CELL AS IS)\n",
    "VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n",
    "BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\n",
    "f1StripesRDD = buildStripes(f1RDD, VOCAB, BASIS).cache()\n",
    "start = time.time()\n",
    "f1StripesRDD.top(5)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - run the full analysis (RUN THIS CELL AS IS)\n",
    "VOCAB = ast.literal_eval(open(\"vocabulary.txt\", \"r\").read())\n",
    "BASIS = ast.literal_eval(open(\"basis.txt\", \"r\").read())\n",
    "stripesRDD = buildStripes(dataRDD, VOCAB, BASIS).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zones\n",
      "['environments', 'gaza', 'uppermost', 'buffer', 'warmer', 'residential', 'remotest', 'adhesion', 'parks', 'localities', 'subdivided', 'saturation']\n",
      "-------\n",
      "zone\n",
      "['inorganic', 'penis', 'tribal', 'cartilage', 'glowing', 'southeastern', 'buffer', 'poorly', 'cracks', 'penetrating', 'trigger', 'assisting', 'subdivided', 'vomiting', 'parked', 'sandy', 'americas', 'excitation', 'au', 'fibrous', 'originate', 'ie', 'turbulent', 'uppermost', 'residential', 'articular', 'persia', 'officially', 'intervening', 'alaska', 'unusually', 'narrower', 'transitional', 'defines', 'atlas', 'flexor', 'illuminated', 'avoidance', 'accumulate', 'contamination', 'masculine', 'auxiliary', 'traversed', 'diffuse', 'guides', 'saturation']\n",
      "-------\n",
      "zinc\n",
      "['phosphorus', 'ammonium', 'metallic', 'weighing', 'burns', 'radioactive', 'dysfunction', 'hydroxide', 'leukemia', 'pancreas', 'wasting', 'insoluble', 'diamond', 'transcription', 'dipped', 'coating', 'dietary', \"alzheimer's\"]\n",
      "-------\n",
      "Wall time: 262.7769560813904 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - take a look at a few stripes (RUN THIS CELL AS IS)\n",
    "# using original solution\n",
    "start = time.time()\n",
    "for wrd, stripe in stripesRDD.top(3):\n",
    "    print(wrd)\n",
    "    print(list(stripe))\n",
    "    print('-------')\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1321.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/media/notebooks/Assignments/HW3/stripes already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-4f56d8755fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# part d - save your full stripes to file for ease of retrival later... (OPTIONAL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstripesRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPWD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/stripes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m             \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1321.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/media/notebooks/Assignments/HW3/stripes already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:283)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1493)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1472)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1472)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# part d - save your full stripes to file for ease of retrival later... (OPTIONAL)\n",
    "stripesRDD.saveAsTextFile(PWD + '/stripes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Synonym Detection\n",
    "\n",
    "We're now ready to perform the main synonym detection analysis. In the tasks below you will compute cosine, jaccard, dice and overlap similarity measurements for each pair of words in our vocabulary and then sort your results to find the most similar pairs of words in this dataset. __`IMPORTANT:`__ When you get to the sorting step please __sort on cosine similarity__ only, so that we can ensure consistent results from student to student. \n",
    "\n",
    "Remember to test each step of your work with the small files before running your code on the full dataset. This is a computationally intense task: well designed code can be the difference between a 20min job and a 2hr job. __`NOTE:`__ _as you are designing your code you may want to review questions 3 and 4 where we modeled some of the key pieces of this analysis._\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ In question 7 you wrote a function that would create word stripes for each `term` in our vocabulary. These word stripes are essentially an 'embedded representation' of the `term`'s meaning. What is the 'feature space' for this representation? (i.e. what are the features of our 1-hot encoded vectors?). What is the maximum length of a stripe?\n",
    "\n",
    "* __b) short response:__ Remember that we are going to treat these stripes as 'documents' and perform similarity analysis on them. The first step is to emit postings which then get collected to form an 'inverted index.' How many rows will there be in our inverted index? Explain.\n",
    "\n",
    "* __c) short response:__ In the demo from question 2, we were able to compute the cosine similarity directly from the stripes (we did this using their vector form, but could have used the list instead). So why do we need the inverted index? (__`HINT:`__ _see your answer to Q4a & Q4b_)\n",
    "\n",
    "* __d) code:__ Write a spark job that does the following:\n",
    "  * loops over the stripes from Q7 and emits postings for the `term` (_remember stripe = document_)\n",
    "  * aggregates the postings to create an inverted index\n",
    "  * loops over all pairs of `term`s that appear in the same inverted index and emits co-occurrence counts\n",
    "  * aggregates co-occurrences\n",
    "  * uses the counts (along with the accompanying information) to compute the cosine, jacard, dice and overlap similarity metrics for each pair of words in the vocabulary \n",
    "  * retrieve the top 20 and bottom 20 most/least similar pairs of words\n",
    "  * also returned the cached sorted RDD for use in the next question  \n",
    "  __`NOTE 1`:__ _Don't forget to include the stripe length when you are creating the postings & co-occurrence pairs. A composite key is the way to go here._  \n",
    "  __`NOTE 2`:__ _Please make sure that your final results are sorted according to cosine similarity otherwise your results may not match the expected result & you will be marked wrong._\n",
    "  \n",
    "* __e) code:__ Comment on the quality of the \"synonyms\" your analysis comes up with. Do you notice anything odd about these pairs of words? Discuss at least one idea for how you might go about improving on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __a)__ The feature space for a stripe is the basis. Therefore the maximum length is the same as the length of the basis which is 1000.\n",
    "\n",
    "> __b)__ A term's inverted index will have as many number of rows as all the terms that co-occured with it \n",
    "\n",
    "> __c)__ In order to parallelize the computation without having to keep the entire co-occurance matrix in memory, we need to have the inverted indices first. With the indices, we can generate records of pairs of terms and their co-occurrence counts and synchronize around the term pairs. Then we can aggregate the co-occurrence counts for a pair of terms and calculate the similarity scores. Without the inverted indices as a by-product, we couldn't have parallelized the computation without keeping the entire co-occurance matrix in memory.\n",
    "\n",
    "> __e)__ Looking at the result of our synonym detection analysis, I was pretty surprised to find that none of the synonyms really made sense. They just look like words that would appear in the same context but not similar enough in meaning. I'm thinking maybe we are not quite at the right place yet in the spectrum of underfitting and overfitting. I think we should try changing where we are getting our basis from in the full ordered list of words based on their counts.  \n",
    "> Especially, as the first attempt, I would go with trying a basis of a set of more rarely seemed words in our corpus since our current result seems to just have very common words as synonyms. By using rarer words as basis, we might be able to evaluate the meaning of a word without letting common words having too many shared co-occuring words and make predictions based on semantics more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for pretty printing (RUN THIS CELL AS IS)\n",
    "def displayOutput(lines):\n",
    "    template = \"{:25}|{:6}, {:7}, {:7}, {:5}\"\n",
    "    print(template.format(\"Pair\", \"Cosine\", \"Jaccard\", \"Overlap\", \"Dice\"))\n",
    "    for pair, scores in lines:\n",
    "        scores = [round(s,4) for s in scores]\n",
    "        print(template.format(pair, *scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`TIP:`__ Feel free to define helper functions within the main function to help you organize your code. Readability is important! Eg:\n",
    "```\n",
    "def similarityAnlysis(stripesRDD):\n",
    "    \"\"\"main docstring\"\"\"\n",
    "    \n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############ YOUR CODE HERE ###########\n",
    "    def helper1():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    def helper2():\n",
    "        \"\"\"helper docstring\"\"\"\n",
    "        return x\n",
    "        \n",
    "    # main spark job starts here\n",
    "    \n",
    "        ...etc\n",
    "    ############ (END) YOUR CODE ###########\n",
    "    return simScoresRDD, top_n, bottom_n\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - write your spark job in the space provided\n",
    "def similarityAnalysis(stripesRDD, n):\n",
    "    \"\"\"\n",
    "    This function defines a Spark DAG to compute cosine, jaccard, \n",
    "    overlap and dice scores for each pair of words in the stripes\n",
    "    provided. \n",
    "    \n",
    "    Output: an RDD, a list of top n, a list of bottom n\n",
    "    \"\"\"\n",
    "    simScoresRDD, top_n, bottom_n = None, None, None\n",
    "    \n",
    "    ############### YOUR CODE HERE ################\n",
    "    def splitStripes(term, cowords):\n",
    "        \"\"\"tokenize each stripe and emit postings.\"\"\"\n",
    "        for cow in cowords:\n",
    "            yield (cow, [(term,len(cowords))])\n",
    "    \n",
    "    def makeCompositeKey(inverted_index):\n",
    "        \"\"\"loop over postings and yield pairs.\"\"\"\n",
    "        term, postings = inverted_index\n",
    "        for subset in itertools.combinations(sorted(postings), 2):\n",
    "            yield (str(subset), 1)\n",
    "    \n",
    "    def similarity(line):\n",
    "        \"\"\"compute similarity scores\"\"\"\n",
    "        (term1, n1), (term2, n2) = ast.literal_eval(line[0])\n",
    "        total = int(line[1])\n",
    "        \n",
    "        cosine = total / np.sqrt(float(n1) * float(n2))\n",
    "        jaccard = total / float(int(n1) + int(n2) - total)\n",
    "        overlap = total / min(int(n1),int(n2))\n",
    "        dice = 2 * total / (int(n1) + int(n2))\n",
    "        yield (term1 +' - '+ term2, (cosine,jaccard,overlap,dice))\n",
    "\n",
    "    \n",
    "    result = stripesRDD.flatMap(lambda x: splitStripes(*x)) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(makeCompositeKey) \\\n",
    "             .reduceByKey(lambda x,y : x+y) \\\n",
    "             .flatMap(similarity) \\\n",
    "             .cache()\n",
    "    \n",
    "    top_n = result.takeOrdered(n, key=lambda x: -x[1][0])\n",
    "    bottom_n = result.takeOrdered(n, key=lambda x: x[1][0])\n",
    "\n",
    "    ############### (END) YOUR CODE ##############\n",
    "    return result, top_n, bottom_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.3992197513580322 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "testResult, top_n, bottom_n = similarityAnalysis(testStripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0.8169105052947998 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "f1Result, top_n, bottom_n = similarityAnalysis(f1StripesRDD, 10)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1234.7617242336273 seconds\n"
     ]
    }
   ],
   "source": [
    "# part d - run the system test (RUN THIS CELL AS IS... use display cell below to see results)\n",
    "start = time.time()\n",
    "result, top_n, bottom_n = similarityAnalysis(stripesRDD, 20)\n",
    "print(\"Wall time: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "first - time             |  0.89,  0.8012,  0.9149, 0.8897\n",
      "time - well              |0.8895,   0.801,   0.892, 0.8895\n",
      "great - time             | 0.875,  0.7757,   0.925, 0.8737\n",
      "part - well              | 0.874,  0.7755,  0.9018, 0.8735\n",
      "first - well             |0.8717,  0.7722,  0.8936, 0.8715\n",
      "part - time              |0.8715,  0.7715,  0.9018, 0.871\n",
      "time - upon              |0.8668,   0.763,  0.9152, 0.8656\n",
      "made - time              | 0.866,  0.7619,  0.9109, 0.8649\n",
      "made - well              |0.8601,  0.7531,  0.9022, 0.8592\n",
      "time - way               |0.8587,  0.7487,  0.9259, 0.8563\n",
      "great - well             |0.8526,  0.7412,  0.8988, 0.8514\n",
      "time - two               |0.8517,  0.7389,  0.9094, 0.8498\n",
      "first - great            |0.8497,  0.7381,  0.8738, 0.8493\n",
      "first - part             |0.8471,  0.7348,  0.8527, 0.8471\n",
      "great - upon             |0.8464,  0.7338,  0.8475, 0.8464\n",
      "upon - well              |0.8444,   0.729,   0.889, 0.8433\n",
      "new - time               |0.8426,   0.724,  0.9133, 0.8399\n",
      "first - two              |0.8411,  0.7249,  0.8737, 0.8405\n",
      "way - well               |0.8357,  0.7146,  0.8986, 0.8335\n",
      "time - us                |0.8357,  0.7105,  0.9318, 0.8308\n",
      "\n",
      "LEAST SIMILAR:\n",
      "Pair                     |Cosine, Jaccard, Overlap, Dice \n",
      "region - write           |0.0067,  0.0032,  0.0085, 0.0065\n",
      "relation - snow          |0.0067,  0.0026,  0.0141, 0.0052\n",
      "cardiac - took           |0.0074,  0.0023,  0.0217, 0.0045\n",
      "ever - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
      "came - tumor             |0.0076,   0.002,  0.0263, 0.004\n",
      "let - therapy            |0.0076,   0.003,  0.0161, 0.0059\n",
      "related - stay           |0.0078,  0.0036,  0.0116, 0.0072\n",
      "factors - hear           |0.0078,  0.0039,  0.0094, 0.0077\n",
      "implications - round     |0.0078,  0.0033,  0.0145, 0.0066\n",
      "came - proteins          |0.0079,   0.002,  0.0286, 0.0041\n",
      "population - window      |0.0079,  0.0039,    0.01, 0.0077\n",
      "love - proportional      | 0.008,  0.0029,  0.0185, 0.0058\n",
      "got - multiple           | 0.008,  0.0034,  0.0149, 0.0067\n",
      "changes - fort           |0.0081,  0.0032,  0.0161, 0.0065\n",
      "layer - wife             |0.0081,  0.0038,  0.0119, 0.0075\n",
      "five - sympathy          |0.0081,  0.0034,  0.0149, 0.0068\n",
      "arrival - essential      |0.0081,   0.004,  0.0093, 0.008\n",
      "desert - function        |0.0081,  0.0031,  0.0175, 0.0062\n",
      "fundamental - stood      |0.0081,  0.0038,  0.0115, 0.0077\n",
      "patients - plain         |0.0081,   0.004,  0.0103, 0.0079\n"
     ]
    }
   ],
   "source": [
    "# part d - display the results (RUN THIS CELL AS IS)\n",
    "print(\"MOST SIMILAR:\")\n",
    "displayOutput(top_n)\n",
    "print(\"\")\n",
    "print(\"LEAST SIMILAR:\")\n",
    "displayOutput(bottom_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__  \n",
    "<table>\n",
    "<th>Most Similar</th>\n",
    "<th>Least Similar</th>\n",
    "<tr><td><pre>\n",
    "Pair           |Cosine, Jaccard, Overlap, Dice \n",
    "first - time   |  0.89,  0.8012,  0.9149, 0.8897\n",
    "time - well    |0.8895,   0.801,   0.892, 0.8895\n",
    "great - time   | 0.875,  0.7757,   0.925, 0.8737\n",
    "part - well    | 0.874,  0.7755,  0.9018, 0.8735\n",
    "first - well   |0.8717,  0.7722,  0.8936, 0.8715\n",
    "part - time    |0.8715,  0.7715,  0.9018, 0.871\n",
    "time - upon    |0.8668,   0.763,  0.9152, 0.8656\n",
    "made - time    | 0.866,  0.7619,  0.9109, 0.8649\n",
    "made - well    |0.8601,  0.7531,  0.9022, 0.8592\n",
    "time - way     |0.8587,  0.7487,  0.9259, 0.8563\n",
    "great - well   |0.8526,  0.7412,  0.8988, 0.8514\n",
    "time - two     |0.8517,  0.7389,  0.9094, 0.8498\n",
    "first - great  |0.8497,  0.7381,  0.8738, 0.8493\n",
    "first - part   |0.8471,  0.7348,  0.8527, 0.8471\n",
    "great - upon   |0.8464,  0.7338,  0.8475, 0.8464\n",
    "upon - well    |0.8444,   0.729,   0.889, 0.8433\n",
    "new - time     |0.8426,   0.724,  0.9133, 0.8399\n",
    "first - two    |0.8411,  0.7249,  0.8737, 0.8405\n",
    "way - well     |0.8357,  0.7146,  0.8986, 0.8335\n",
    "time - us      |0.8357,  0.7105,  0.9318, 0.8308\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "Pair                  |Cosine, Jaccard, Overlap, Dice \n",
    "region - write        |0.0067,  0.0032,  0.0085, 0.0065\n",
    "relation - snow       |0.0067,  0.0026,  0.0141, 0.0052\n",
    "cardiac - took        |0.0074,  0.0023,  0.0217, 0.0045\n",
    "ever - tumor          |0.0076,   0.002,  0.0263, 0.004\n",
    "came - tumor          |0.0076,   0.002,  0.0263, 0.004\n",
    "let - therapy         |0.0076,   0.003,  0.0161, 0.0059\n",
    "related - stay        |0.0078,  0.0036,  0.0116, 0.0072\n",
    "factors - hear        |0.0078,  0.0039,  0.0094, 0.0077\n",
    "implications - round  |0.0078,  0.0033,  0.0145, 0.0066\n",
    "came - proteins       |0.0079,   0.002,  0.0286, 0.0041\n",
    "population - window   |0.0079,  0.0039,    0.01, 0.0077\n",
    "love - proportional   | 0.008,  0.0029,  0.0185, 0.0058\n",
    "got - multiple        | 0.008,  0.0034,  0.0149, 0.0067\n",
    "changes - fort        |0.0081,  0.0032,  0.0161, 0.0065\n",
    "layer - wife          |0.0081,  0.0038,  0.0119, 0.0075\n",
    "five - sympathy       |0.0081,  0.0034,  0.0149, 0.0068\n",
    "arrival - essential   |0.0081,   0.004,  0.0093, 0.008\n",
    "desert - function     |0.0081,  0.0031,  0.0175, 0.0062\n",
    "fundamental - stood   |0.0081,  0.0038,  0.0115, 0.0077\n",
    "patients - plain      |0.0081,   0.004,  0.0103, 0.0079\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you have completed HW3! Please refer to the readme for submission instructions.\n",
    "\n",
    "If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLScgIz4laP2JHChStLZx8MO0jGvrGyrOyQBnj7M4_4vcVXkB7g/viewform?usp=sf_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "441px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "827px",
    "left": "0px",
    "right": "1125px",
    "top": "107px",
    "width": "428px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
